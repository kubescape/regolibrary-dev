{
    "name": "security",
    "description": "Controls that are used to assess security threats.",
    "attributes": {
        "builtin": true
    },
    "typeTags": [
        "security"
    ],
    "scanningScope": {
        "matches": [
            "cluster",
            "file"
        ]
    },
    "version": null,
    "controls": [
        {
            "name": "API server insecure port is enabled",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "Kubernetes control plane API is running with non-secure port enabled which allows attackers to gain unprotected access to the cluster.",
            "remediation": "Set the insecure-port flag of the API server to zero.",
            "long_description": "The control plane is the core of Kubernetes and gives users the ability to view containers, schedule new Pods, read Secrets, and execute commands in the cluster. Therefore, it should be protected. It is recommended to avoid control plane exposure to the Internet or to an untrusted network. The API server runs on ports 6443 and 8080. We recommend to block them in the firewall. Note also that port 8080, when accessed through the local machine, does not require TLS encryption, and the requests bypass authentication and authorization modules.",
            "test": "Check if the insecure-port flag is set (in case of cloud vendor hosted Kubernetes service this verification will not be effective).",
            "controlID": "C-0005",
            "category": {
                "name": "Control plane",
                "id": "Cat-1"
            },
            "baseScore": 9,
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "insecure-port-flag",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if the api server has insecure-port enabled",
                    "remediation": "Make sure that the insecure-port flag of the api server is set to 0",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport data.cautils\n\n# Fails if pod has insecure-port flag enabled\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontains(pod.metadata.name, \"kube-apiserver\")\n    container := pod.spec.containers[i]\n\tpath = is_insecure_port_flag(container, i)\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"The API server container: %v has insecure-port flag enabled\", [ container.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\": [],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\nis_insecure_port_flag(container, i) = path {\n\tcommand := container.command[j]\n\tcontains(command, \"--insecure-port=1\")\n\tpath := sprintf(\"spec.containers[%v].command[%v]\", [format_int(i, 10), format_int(j, 10)])\n}",
                    "resourceEnumerator": "package armo_builtins\n\nimport data.cautils\n\n# Fails if pod has insecure-port flag enabled\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontains(pod.metadata.name, \"kube-apiserver\")\n    container := pod.spec.containers[_]\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"The API server container: %v has insecure-port flag enabled\", [container.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n"
                }
            ]
        },
        {
            "name": "Applications credentials in configuration files",
            "attributes": {
                "actionRequired": "configuration",
                "microsoftMitreColumns": [
                    "Credential access",
                    "Lateral Movement"
                ],
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "security-impact"
                ]
            },
            "description": "Attackers who have access to configuration files can steal the stored secrets and use them. This control checks if ConfigMaps or pod specifications have sensitive information in their configuration.",
            "remediation": "Use Kubernetes secrets or Key Management Systems to store credentials.",
            "long_description": "Developers store secrets in the Kubernetes configuration files, such as environment variables in the pod configuration. Such behavior is commonly seen in clusters that are monitored by Azure Security Center. Attackers who have access to those configurations, by querying the API server or by accessing those files on the developer\u2019s endpoint, can steal the stored secrets and use them.",
            "test": "Check if the pod has sensitive information in environment variables, by using list of known sensitive key names. Check if there are configmaps with sensitive information.",
            "controlID": "C-0012",
            "baseScore": 8.0,
            "category": {
                "name": "Secrets",
                "id": "Cat-3"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "rule-credentials-in-env-var",
                    "attributes": {
                        "m$K8sThreatMatrix": "Credential access::Applications credentials in configuration files, Lateral Movement::Applications credentials in configuration files"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "configInputs": [
                        "settings.postureControlInputs.sensitiveValues",
                        "settings.postureControlInputs.sensitiveKeyNames",
                        "settings.postureControlInputs.sensitiveValuesAllowed",
                        "settings.postureControlInputs.sensitiveKeyNamesAllowed"
                    ],
                    "controlConfigInputs": [
                        {
                            "path": "settings.postureControlInputs.sensitiveValues",
                            "name": "Sensitive Values",
                            "description": "Strings that identify a value that Kubescape believes should be stored in a Secret, and not in a ConfigMap or an environment variable."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveValuesAllowed",
                            "name": "Allowed Values",
                            "description": "Reduce false positives with known values."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveKeyNames",
                            "name": "Sensitive Keys",
                            "description": "Key names that identify a potential value that should be stored in a Secret, and not in a ConfigMap or an environment variable."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveKeyNamesAllowed",
                            "name": "Allowed Keys",
                            "description": "Reduce false positives with known key names."
                        }
                    ],
                    "description": "fails if Pods have sensitive information in configuration",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "\tpackage armo_builtins\n\n\tdeny[msga] {\n\t\tpod := input[_]\n\t\tpod.kind == \"Pod\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_key_names := data.postureControlInputs.sensitiveKeyNames\n\t\tkey_name := sensitive_key_names[_]\n\t\tcontainer := pod.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.name), lower(key_name))\n\t\tenv.value != \"\"\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Pod: %v has sensitive information in environment variables\", [pod.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [pod]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\t\tspec_template_spec_patterns[wl.kind]\n\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_key_names := data.postureControlInputs.sensitiveKeyNames\n\t\tkey_name := sensitive_key_names[_]\n\t\tcontainer := wl.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.name), lower(key_name))\n\t\tenv.value != \"\"\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\tsprintf(\"spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"%v: %v has sensitive information in environment variables\", [wl.kind, wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\twl.kind == \"CronJob\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_key_names := data.postureControlInputs.sensitiveKeyNames\n\t\tkey_name := sensitive_key_names[_]\n\t\tcontainer := wl.spec.jobTemplate.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.name), lower(key_name))\n\t\tenv.value != \"\"\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Cronjob: %v has sensitive information in environment variables\", [wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n# check sensitive values\ndeny[msga] {\n\t\tpod := input[_]\n\t\tpod.kind == \"Pod\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_values := data.postureControlInputs.sensitiveValues\n    \tvalue := sensitive_values[_]\n\t\tcontainer := pod.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.value), lower(value))\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Pod: %v has sensitive information in environment variables\", [pod.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [pod]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\t\tspec_template_spec_patterns[wl.kind]\n\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_values := data.postureControlInputs.sensitiveValues\n    \tvalue := sensitive_values[_]\n\t\tcontainer := wl.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.value), lower(value))\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\tsprintf(\"spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"%v: %v has sensitive information in environment variables\", [wl.kind, wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\twl.kind == \"CronJob\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_values := data.postureControlInputs.sensitiveValues\n    \tvalue := sensitive_values[_]\n\t\tcontainer := wl.spec.jobTemplate.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.value), lower(value))\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Cronjob: %v has sensitive information in environment variables\", [wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n\nis_not_reference(env)\n{\n\tnot env.valueFrom.secretKeyRef\n\tnot env.valueFrom.configMapKeyRef\n}\n\nis_allowed_value(value) {\n    allow_val := data.postureControlInputs.sensitiveValuesAllowed[_]\n    regex.match(allow_val , value)\n}\n\nis_allowed_key_name(key_name) {\n    allow_key := data.postureControlInputs.sensitiveKeyNamesAllowed[_]\n    contains(lower(key_name), lower(allow_key))\n}"
                },
                {
                    "name": "rule-credentials-configmap",
                    "attributes": {
                        "m$K8sThreatMatrix": "Credential access::Applications credentials in configuration files, Lateral Movement::Applications credentials in configuration files"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                "*"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "ConfigMap"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "configInputs": [
                        "settings.postureControlInputs.sensitiveValues",
                        "settings.postureControlInputs.sensitiveKeyNames",
                        "settings.postureControlInputs.sensitiveValuesAllowed",
                        "settings.postureControlInputs.sensitiveKeyNamesAllowed"
                    ],
                    "controlConfigInputs": [
                        {
                            "path": "settings.postureControlInputs.sensitiveValues",
                            "name": "Sensitive Values",
                            "description": "Strings that identify a value that Kubescape believes should be stored in a Secret, and not in a ConfigMap or an environment variable."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveValuesAllowed",
                            "name": "Allowed Values",
                            "description": "Reduce false positives with known values."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveKeyNames",
                            "name": "Sensitive Keys",
                            "description": "Key names that identify a potential value that should be stored in a Secret, and not in a ConfigMap or an environment variable."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveKeyNamesAllowed",
                            "name": "Allowed Keys",
                            "description": "Reduce false positives with known key names."
                        }
                    ],
                    "description": "fails if ConfigMaps have sensitive information in configuration",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# fails if config map has keys with suspicious name\ndeny[msga] {\n\tconfigmap := input[_]\n    configmap.kind == \"ConfigMap\"\n    # see default-config-inputs.json for list values\n    sensitive_key_names := data.postureControlInputs.sensitiveKeyNames\n    key_name := sensitive_key_names[_]\n    map_secret := configmap.data[map_key]\n    map_secret != \"\"\n\n    contains(lower(map_key), lower(key_name))\n\n    # check that value or key weren't allowed by user\n    not is_allowed_value(map_secret)\n    not is_allowed_key_name(map_key)\n\n    path := sprintf(\"data[%v]\", [map_key])\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"this configmap has sensitive information: %v\", [configmap.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n        \"failedPaths\": [path],\n        \"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n          \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [configmap]\n\t\t}\n     }\n}\n\n# fails if config map has values with suspicious content - not base 64\ndeny[msga] {\n    # see default-config-inputs.json for list values\n    sensitive_values := data.postureControlInputs.sensitiveValues\n    value := sensitive_values[_]\n\n\tconfigmap := input[_]\n    configmap.kind == \"ConfigMap\"\n    map_secret := configmap.data[map_key]\n    map_secret != \"\"\n\n    regex.match(value , map_secret)\n\n    # check that value or key weren't allowed by user\n    not is_allowed_value(map_secret)\n    not is_allowed_key_name(map_key)\n\n    path := sprintf(\"data[%v]\", [map_key])\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"this configmap has sensitive information: %v\", [configmap.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n       \"failedPaths\": [path],\n        \"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n          \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [configmap]\n\t\t}\n     }\n}\n\n# fails if config map has values with suspicious content - base 64\ndeny[msga] {\n    # see default-config-inputs.json for list values\n    sensitive_values := data.postureControlInputs.sensitiveValues\n    value := sensitive_values[_]\n\n\tconfigmap := input[_]\n    configmap.kind == \"ConfigMap\"\n    map_secret := configmap.data[map_key]\n    map_secret != \"\"\n\n    decoded_secret := base64.decode(map_secret)\n\n    regex.match(value , decoded_secret)\n\n    # check that value or key weren't allowed by user\n    not is_allowed_value(map_secret)\n    not is_allowed_key_name(map_key)\n\n    path := sprintf(\"data[%v]\", [map_key])\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"this configmap has sensitive information: %v\", [configmap.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n       \"failedPaths\": [path],\n        \"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n          \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [configmap]\n\t\t}\n     }\n}\n\nis_allowed_value(value) {\n    allow_val := data.postureControlInputs.sensitiveValuesAllowed[_]\n    regex.match(allow_val , value)\n}\n\nis_allowed_key_name(key_name) {\n    allow_key := data.postureControlInputs.sensitiveKeyNamesAllowed[_]\n    contains(lower(key_name), lower(allow_key))\n}"
                }
            ]
        },
        {
            "name": "Non-root containers",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "Potential attackers may gain access to a container and leverage its existing privileges to conduct an attack. Therefore, it is not recommended to deploy containers with root privileges unless it is absolutely necessary. This control identifies all the pods running as root or can escalate to root.",
            "remediation": "If your application does not need root privileges, make sure to define runAsNonRoot as true or explicitly set the runAsUser using ID 1000 or higher under the PodSecurityContext or container securityContext. In addition, set an explicit value for runAsGroup using ID 1000 or higher.",
            "long_description": "Container engines allow containers to run applications as a non-root user with non-root group membership. Typically, this non-default setting is configured when the container image is built. Alternatively, Kubernetes can load containers into a Pod with SecurityContext:runAsUser specifying a non-zero user. While the runAsUser directive effectively forces non-root execution at deployment, NSA and CISA encourage developers to build container applications to execute as a non-root user. Having non-root execution integrated at build time provides better assurance that applications will function correctly without root privileges.",
            "test": "Verify that runAsUser is set to a user id greater than 0 or that runAsNonRoot is set to true, and that runAsGroup is set to an id greater than 0. Check all the combinations with PodSecurityContext and SecurityContext (for containers).",
            "controlID": "C-0013",
            "baseScore": 6.0,
            "example": "@controls/examples/c013.yaml",
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Node escape",
                    "id": "Cat-9"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "non-root-containers",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container can run as root",
                    "remediation": "Make sure that the user/group in the securityContext of pod/container is set to an id over 0, or the runAsNonRoot flag is set to true.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n################################################################################\n# Rules\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\n\tstart_of_path := \"spec\"\n\trun_as_user_fixpath := evaluate_workload_run_as_user(container, pod, start_of_path)\n\trun_as_group_fixpath := evaluate_workload_run_as_group(container, pod, start_of_path)\n\tall_fixpaths := array.concat(run_as_user_fixpath, run_as_group_fixpath)\n\tcount(all_fixpaths) > 0\n\tfixPaths := get_fixed_paths(all_fixpaths, i)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in pod: %v  may run as root\", [container.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n        \"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n\n\tstart_of_path := \"spec.template.spec\"\n\trun_as_user_fixpath := evaluate_workload_run_as_user(container, wl.spec.template, start_of_path)\n\trun_as_group_fixpath := evaluate_workload_run_as_group(container, wl.spec.template, start_of_path)\n\tall_fixpaths := array.concat(run_as_user_fixpath, run_as_group_fixpath)\n\tcount(all_fixpaths) > 0\n\tfixPaths := get_fixed_paths(all_fixpaths, i)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in %v: %v may run as root\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n        \"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if cronjob has a container configured to run as root\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec\"\n\trun_as_user_fixpath := evaluate_workload_run_as_user(container, wl.spec.jobTemplate.spec.template, start_of_path)\n\trun_as_group_fixpath := evaluate_workload_run_as_group(container, wl.spec.jobTemplate.spec.template, start_of_path)\n\tall_fixpaths := array.concat(run_as_user_fixpath, run_as_group_fixpath)\n\tcount(all_fixpaths) > 0\n\tfixPaths := get_fixed_paths(all_fixpaths, i)\n\t\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in %v: %v  may run as root\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n        \"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\nget_fixed_paths(all_fixpaths, i) = [{\"path\":replace(all_fixpaths[0].path,\"container_ndx\",format_int(i,10)), \"value\":all_fixpaths[0].value}, {\"path\":replace(all_fixpaths[1].path,\"container_ndx\",format_int(i,10)), \"value\":all_fixpaths[1].value}]{\n\tcount(all_fixpaths) == 2\n} else = [{\"path\":replace(all_fixpaths[0].path,\"container_ndx\",format_int(i,10)), \"value\":all_fixpaths[0].value}] \n\n#################################################################################\n# Workload evaluation \n\n# if runAsUser is set to 0 and runAsNonRoot is set to false/ not set - suggest to set runAsUser to 1000\n# if runAsUser is not set and runAsNonRoot is set to false/ not set - suggest to set runAsNonRoot to true\n# all checks are both on the pod and the container level\nevaluate_workload_run_as_user(container, pod, start_of_path) = fixPath {\n\trunAsNonRootValue := get_run_as_non_root_value(container, pod, start_of_path)\n\trunAsNonRootValue.value == false\n\t\n\trunAsUserValue := get_run_as_user_value(container, pod, start_of_path)\n\trunAsUserValue.value == 0\n\n\talertInfo := choose_first_if_defined(runAsUserValue, runAsNonRootValue)\n\tfixPath := alertInfo.fixPath\n} else = [] \n\n\n# if runAsGroup is set to 0/ not set - suggest to set runAsGroup to 1000\n# all checks are both on the pod and the container level\nevaluate_workload_run_as_group(container, pod, start_of_path) = fixPath {\t\n\trunAsGroupValue := get_run_as_group_value(container, pod, start_of_path)\n\trunAsGroupValue.value == 0\n\n\tfixPath := runAsGroupValue.fixPath\n} else = []\n\n\n#################################################################################\n# Value resolution functions\n\n\nget_run_as_non_root_value(container, pod, start_of_path) = runAsNonRoot {\n    runAsNonRoot := {\"value\" : container.securityContext.runAsNonRoot, \"fixPath\": [{\"path\": sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]), \"value\":\"true\"}], \"defined\" : true}\n} else = runAsNonRoot {\n    runAsNonRoot := {\"value\" : pod.spec.securityContext.runAsNonRoot, \"fixPath\": [{\"path\": sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]), \"value\":\"true\"}], \"defined\" : true}\n}  else = {\"value\" : false, \"fixPath\": [{\"path\":  sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]) , \"value\":\"true\"}], \"defined\" : false}\n\nget_run_as_user_value(container, pod, start_of_path) = runAsUser {\n\tpath := sprintf(\"%v.containers[container_ndx].securityContext.runAsUser\", [start_of_path]) \n    runAsUser := {\"value\" : container.securityContext.runAsUser, \"fixPath\": [{\"path\": path, \"value\": \"1000\"}], \"defined\" : true}\n} else = runAsUser {\n\tpath := sprintf(\"%v.securityContext.runAsUser\", [start_of_path]) \n    runAsUser := {\"value\" : pod.spec.securityContext.runAsUser, \"fixPath\": [{\"path\": path, \"value\": \"1000\"}],\"defined\" : true}\n} else = {\"value\" : 0, \"fixPath\": [{\"path\":  sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]), \"value\":\"true\"}],\n\t\"defined\" : false}\n\nget_run_as_group_value(container, pod, start_of_path) = runAsGroup {\n\tpath := sprintf(\"%v.containers[container_ndx].securityContext.runAsGroup\", [start_of_path])\n    runAsGroup := {\"value\" : container.securityContext.runAsGroup, \"fixPath\": [{\"path\": path, \"value\": \"1000\"}],\"defined\" : true}\n} else = runAsGroup {\n\tpath := sprintf(\"%v.securityContext.runAsGroup\", [start_of_path])\n    runAsGroup := {\"value\" : pod.spec.securityContext.runAsGroup, \"fixPath\":[{\"path\": path, \"value\": \"1000\"}], \"defined\" : true}\n} else = {\"value\" : 0, \"fixPath\": [{\"path\": sprintf(\"%v.containers[container_ndx].securityContext.runAsGroup\", [start_of_path]), \"value\":\"1000\"}],\n \t\"defined\" : false\n}\n\nchoose_first_if_defined(l1, l2) = c {\n    l1.defined\n    c := l1\n} else = l2\n\n"
                }
            ]
        },
        {
            "name": "Allow privilege escalation",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "smartRemediation"
                ]
            },
            "description": "Attackers may gain access to a container and uplift its privilege to enable excessive capabilities.",
            "remediation": "If your application does not need it, make sure the allowPrivilegeEscalation field of the securityContext is set to false.",
            "test": " Check that the allowPrivilegeEscalation field in securityContext of container is set to false.   ",
            "controlID": "C-0016",
            "baseScore": 6.0,
            "example": "@controls/examples/allowprivilegeescalation.yaml",
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Node escape",
                    "id": "Cat-9"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "rule-allow-privilege-escalation",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "policy"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "PodSecurityPolicy"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container allows privilege escalation",
                    "remediation": "Make sure that the allowPrivilegeEscalation field in the securityContext of pod/container is set to false",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# Fails if pod has container  that allow privilege escalation\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n    is_allow_privilege_escalation_container(container)\n\tfixPath := get_fix_path(i, start_of_path)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in pod: %v  allow privilege escalation\", [container.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPath,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n\n# Fails if workload has a container that allow privilege escalation\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n    is_allow_privilege_escalation_container(container)\n\tfixPath := get_fix_path(i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v  allow privilege escalation\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPath,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if cronjob has a container that allow privilege escalation\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n\tis_allow_privilege_escalation_container(container)\n\tfixPath := get_fix_path(i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v allow privilege escalation\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPath,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n\nis_allow_privilege_escalation_container(container) {\n    not container.securityContext.allowPrivilegeEscalation == false\n\tnot container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) == 0\n}\n\nis_allow_privilege_escalation_container(container) {\n    not container.securityContext.allowPrivilegeEscalation == false\n\tnot container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) > 0\n\tpsp := psps[_]\n\tnot psp.spec.allowPrivilegeEscalation == false\n}\n\n\nis_allow_privilege_escalation_container(container) {\n    container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) == 0\n}\n\nis_allow_privilege_escalation_container(container) {\n    container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) > 0\n\tpsp := psps[_]\n\tnot psp.spec.allowPrivilegeEscalation == false\n}\n\nget_fix_path(i, start_of_path) = [{\"path\": sprintf(\"%vcontainers[%v].securityContext.allowPrivilegeEscalation\", [start_of_path, i]), \"value\":\"false\"},\n\t{\"path\": sprintf(\"%vcontainers[%v].securityContext.privileged\", [start_of_path, i]), \"value\":\"false\"}]\n"
                }
            ]
        },
        {
            "name": "Immutable container filesystem",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "smartRemediation"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Persistence"
                        ]
                    }
                ]
            },
            "description": "Mutable container filesystem can be abused to inject malicious code or data into containers. Use immutable (read-only) filesystem to limit potential attacks.",
            "remediation": "Set the filesystem of the container to read-only when possible (pod securityContext, readOnlyRootFilesystem: true). If containers application needs to write into the filesystem, it is recommended to mount secondary filesystems for specific directories where application require write access.",
            "long_description": "By default, containers are permitted mostly unrestricted execution within their own context. An attacker who has access to a container, can create files and download scripts as he wishes, and modify the underlying application running on the container. ",
            "test": "Check whether the readOnlyRootFilesystem field in the SecurityContext is set to true. ",
            "controlID": "C-0017",
            "baseScore": 3.0,
            "example": "@controls/examples/c017.yaml",
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Node escape",
                    "id": "Cat-9"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "immutable-container-filesystem",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container has mutable filesystem",
                    "remediation": "Make sure that the securityContext.readOnlyRootFilesystem field in the container/pod spec is set to true",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# Fails if pods has container with mutable filesystem\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n    is_mutable_filesystem(container)\n\tfixPath = {\"path\": sprintf(\"%vcontainers[%d].securityContext.readOnlyRootFilesystem\", [start_of_path, i]), \"value\": \"true\"}\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in pod: %v  has  mutable filesystem\", [container.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [fixPath],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# Fails if workload has  container with mutable filesystem \ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n    is_mutable_filesystem(container)\n\tfixPath = {\"path\": sprintf(\"%vcontainers[%d].securityContext.readOnlyRootFilesystem\", [start_of_path, i]), \"value\": \"true\"}\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v has  mutable filesystem\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [fixPath],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if cronjob has  container with mutable filesystem \ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n\tis_mutable_filesystem(container)\n\tfixPath = {\"path\": sprintf(\"%vcontainers[%d].securityContext.readOnlyRootFilesystem\", [start_of_path, i]), \"value\": \"true\"}\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v has mutable filesystem\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [fixPath],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Default of readOnlyRootFilesystem is false. This field is only in container spec and not pod spec\nis_mutable_filesystem(container) {\n\tcontainer.securityContext.readOnlyRootFilesystem == false\n}\n\nis_mutable_filesystem(container) {\n\tnot container.securityContext.readOnlyRootFilesystem == false\n    not container.securityContext.readOnlyRootFilesystem == true\n}\n"
                }
            ]
        },
        {
            "name": "Automatic mapping of service account",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "smartRemediation"
                ]
            },
            "description": "Potential attacker may gain access to a pod and steal its service account token. Therefore, it is recommended to disable automatic mapping of the service account tokens in service account configuration and enable it only for pods that need to use them.",
            "remediation": "Disable automatic mounting of service account tokens to pods either at the service account level or at the individual pod level, by specifying the automountServiceAccountToken: false. Note that pod level takes precedence.",
            "long_description": "We have it in Armo best (Automatic mapping of service account token).",
            "test": "Check all service accounts on which automount is not disabled.  Check all workloads on which they and their service account don't disable automount ",
            "controlID": "C-0034",
            "baseScore": 6.0,
            "example": "@controls/examples/c034.yaml",
            "category": {
                "name": "Secrets",
                "id": "Cat-3"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "automount-service-account",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "ServiceAccount"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if service account and workloads mount service account token by default",
                    "remediation": "Make sure that the automountServiceAccountToken field on the service account spec if set to false",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# Fails if user account mount tokens in pod by default\ndeny [msga]{\n    service_accounts := [service_account |  service_account= input[_]; service_account.kind == \"ServiceAccount\"]\n    service_account := service_accounts[_]\n    result := is_auto_mount(service_account)\n\tfailed_path := get_failed_path(result)\n    fixed_path := get_fixed_path(result)\n\n    msga := {\n\t    \"alertMessage\": sprintf(\"the following service account: %v in the following namespace: %v mounts service account tokens in pods by default\", [service_account.metadata.name, service_account.metadata.namespace]),\n\t\t\"alertScore\": 9,\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"fixPaths\": fixed_path,\n\t\t\"deletePaths\": failed_path,\n\t\t\"failedPaths\": failed_path,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [service_account]\n\t\t}\n\t}\n}    \n\n\n #  -- ----     For workloads     -- ----   \n# Fails if pod mount tokens  by default (either by its config or by its SA config)\n\n # POD  \ndeny [msga]{\n    pod := input[_]\n\tpod.kind == \"Pod\"\n\n\tstart_of_path := \"spec.\"\n\twl_namespace := pod.metadata.namespace\n\tresult := is_sa_auto_mounted(pod.spec, start_of_path, wl_namespace)\n\tfailed_path := get_failed_path(result)\n    fixed_path := get_fixed_path(result)\n\n    msga := {\n\t    \"alertMessage\": sprintf(\"Pod: %v in the following namespace: %v mounts service account tokens by default\", [pod.metadata.name, pod.metadata.namespace]),\n\t\t\"alertScore\": 9,\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"fixPaths\": fixed_path,\n\t\t\"deletePaths\": failed_path,\n\t\t\"failedPaths\": failed_path,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}    \n\n# WORKLOADS\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n\tstart_of_path := \"spec.template.spec.\"\n\n\twl_namespace := wl.metadata.namespace\n\tresult := is_sa_auto_mounted(wl.spec.template.spec, start_of_path, wl_namespace)\n\tfailed_path := get_failed_path(result)\n    fixed_path := get_fixed_path(result)\n\n\tmsga := {\n\t\t\"alertMessage\":  sprintf(\"%v: %v in the following namespace: %v mounts service account tokens by default\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixed_path,\n\t\t\"deletePaths\": failed_path,\n\t\t\"failedPaths\": failed_path,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# CRONJOB\ndeny[msga] {\n  \twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n   \n\twl_namespace := wl.metadata.namespace\n\tresult := is_sa_auto_mounted(wl.spec.jobTemplate.spec.template.spec, start_of_path, wl.metadata)\n\tfailed_path := get_failed_path(result)\n    fixed_path := get_fixed_path(result)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v in the following namespace: %v mounts service account tokens by default\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixed_path,\n\t\t\"deletePaths\": failed_path,\n\t\t\"failedPaths\": failed_path,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n\n #  -- ----     For workloads     -- ----     \nis_sa_auto_mounted(spec, start_of_path, wl_metadata) = [failed_path, fix_path]   {\n\t# automountServiceAccountToken not in pod spec\n\tnot spec.automountServiceAccountToken == false\n\tnot spec.automountServiceAccountToken == true\n\n\t# check if SA  automount by default\n\tsa := input[_]\n\tis_same_sa(spec, sa.metadata.name)\n\tis_same_namespace(sa.metadata , wl_metadata)\n\tnot sa.automountServiceAccountToken == false\n\n\t# path is pod spec\n\tfix_path = { \"path\": sprintf(\"%vautomountServiceAccountToken\", [start_of_path]), \"value\": \"false\"}\n\tfailed_path = \"\"\n}\n\nget_failed_path(paths) = [paths[0]] {\n\tpaths[0] != \"\"\n} else = []\n\n\nget_fixed_path(paths) = [paths[1]] {\n\tpaths[1] != \"\"\n} else = []\n\nis_sa_auto_mounted(spec, start_of_path, wl_namespace) =  [failed_path, fix_path]  {\n\t# automountServiceAccountToken set to true in pod spec\n\tspec.automountServiceAccountToken == true\n\t\n\t# SA automount by default\n\tservice_accounts := [service_account | service_account = input[_]; service_account.kind == \"ServiceAccount\"]\n\tcount(service_accounts) > 0\n\tsa := service_accounts[_]\n\tis_same_sa(spec, sa.metadata.name)\n\tis_same_namespace(sa.metadata , wl_namespace)\n\tnot sa.automountServiceAccountToken == false\n\n\tfailed_path = sprintf(\"%vautomountServiceAccountToken\", [start_of_path])\n\tfix_path = \"\"\n}\n\nis_sa_auto_mounted(spec, start_of_path, wl_namespace) =  [failed_path, fix_path]  {\n\t# automountServiceAccountToken set to true in pod spec\n\tspec.automountServiceAccountToken == true\n\t\n\t# No SA (yaml scan)\n\tservice_accounts := [service_account | service_account = input[_]; service_account.kind == \"ServiceAccount\"]\n\tcount(service_accounts) == 0\n\tfailed_path = sprintf(\"%vautomountServiceAccountToken\", [start_of_path])\n\tfix_path = \"\"\n}\n\n\n\n #  -- ----     For SAs     -- ----     \nis_auto_mount(service_account)  =  [failed_path, fix_path]  {\n\tservice_account.automountServiceAccountToken == true\n\tfailed_path = \"automountServiceAccountToken\"\n\tfix_path = \"\"\n}\n\nis_auto_mount(service_account)=  [failed_path, fix_path]  {\n\tnot service_account.automountServiceAccountToken == false\n\tnot service_account.automountServiceAccountToken == true\n\tfix_path = {\"path\": \"automountServiceAccountToken\", \"value\": \"false\"}\n\tfailed_path = \"\"\n}\n\nis_same_sa(spec, serviceAccountName) {\n\tspec.serviceAccountName == serviceAccountName\n}\n\nis_same_sa(spec, serviceAccountName) {\n\tnot spec.serviceAccountName \n\tserviceAccountName == \"default\"\n}\n\n\nis_same_namespace(metadata1, metadata2) {\n\tmetadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tnot metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata2.namespace\n\tmetadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tmetadata2.namespace == \"default\"\n}"
                }
            ]
        },
        {
            "name": "Administrative Roles",
            "attributes": {
                "microsoftMitreColumns": [
                    "Privilege escalation"
                ],
                "rbacQuery": "Show cluster_admin",
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "Attackers who have cluster admin permissions (can perform any action on any resource), can take advantage of their privileges for malicious activities. This control determines which subjects have cluster admin permissions.",
            "remediation": "You should apply least privilege principle. Make sure cluster admin permissions are granted only when it is absolutely necessary. Don't use subjects with such high permissions for daily operations.",
            "long_description": "Role-based access control (RBAC) is a key security feature in Kubernetes. RBAC can restrict the allowed actions of the various identities in the cluster. Cluster-admin is a built-in high privileged role in Kubernetes. Attackers who have permissions to create bindings and cluster-bindings in the cluster can create a binding to the cluster-admin ClusterRole or to other high privileges roles.",
            "test": "Check which subjects have cluster-admin RBAC permissions \u2013 either by being bound to the cluster-admin clusterrole, or by having equivalent high privileges.  ",
            "controlID": "C-0035",
            "baseScore": 6.0,
            "category": {
                "name": "Access control",
                "id": "Cat-2"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "rule-list-all-cluster-admins-v1",
                    "attributes": {
                        "m$K8sThreatMatrix": "Privilege Escalation::Cluster-admin binding",
                        "resourcesAggregator": "subject-role-rolebinding",
                        "useFromKubescapeVersion": "v1.0.133"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                "rbac.authorization.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Role",
                                "ClusterRole",
                                "ClusterRoleBinding",
                                "RoleBinding"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "determines which users have cluster admin permissions",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.in\n\n# returns subjects with cluster admin permissions\ndeny[msga] {\n\tsubjectVector := input[_]\n\trole := subjectVector.relatedObjects[i]\n\trolebinding := subjectVector.relatedObjects[j]\n\tendswith(role.kind, \"Role\")\n\tendswith(rolebinding.kind, \"Binding\")\n\n\trule := role.rules[p]\n\tsubject := rolebinding.subjects[k]\n\tis_same_subjects(subjectVector, subject)\n\nis_same_subjects(subjectVector, subject)\n\trule_path := sprintf(\"relatedObjects[%d].rules[%d]\", [i, p])\n\n\tverbs := [\"*\"]\n\tverb_path := [sprintf(\"%s.verbs[%d]\", [rule_path, l]) | verb = rule.verbs[l]; verb in verbs]\n\tcount(verb_path) > 0\n\n\tapi_groups := [\"*\", \"\"]\n\tapi_groups_path := [sprintf(\"%s.apiGroups[%d]\", [rule_path, a]) | apiGroup = rule.apiGroups[a]; apiGroup in api_groups]\n\tcount(api_groups_path) > 0\n\n\tresources := [\"*\"]\n\tresources_path := [sprintf(\"%s.resources[%d]\", [rule_path, l]) | resource = rule.resources[l]; resource in resources]\n\tcount(resources_path) > 0\n\n\tpath := array.concat(resources_path, verb_path)\n\tpath2 := array.concat(path, api_groups_path)\n\tfinalpath := array.concat(path2, [\n\t\tsprintf(\"relatedObjects[%d].subjects[%d]\", [j, k]),\n\t\tsprintf(\"relatedObjects[%d].roleRef.name\", [j]),\n\t])\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Subject: %s-%s have high privileges, such as cluster-admin\", [subjectVector.kind, subjectVector.name]),\n\t\t\"alertScore\": 3,\n\t\t\"fixPaths\": [],\n\t\t\"reviewPaths\": finalpath,\n\t\t\"failedPaths\": finalpath,\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [],\n\t\t\t\"externalObjects\": subjectVector,\n\t\t},\n\t}\n}\n\n# for service accounts\nis_same_subjects(subjectVector, subject) {\n\tsubjectVector.kind == subject.kind\n\tsubjectVector.name == subject.name\n\tsubjectVector.namespace == subject.namespace\n}\n\n# for users/ groups\nis_same_subjects(subjectVector, subject) {\n\tsubjectVector.kind == subject.kind\n\tsubjectVector.name == subject.name\n\tsubjectVector.apiGroup == subject.apiGroup\n}\n"
                }
            ]
        },
        {
            "name": "Host PID/IPC privileges",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "Containers should be isolated from the host machine as much as possible. The hostPID and hostIPC fields in deployment yaml may allow cross-container influence and may expose the host itself to potentially malicious or destructive actions. This control identifies all pods using hostPID or hostIPC privileges.",
            "remediation": "Remove hostPID and hostIPC from the yaml file(s) privileges unless they are absolutely necessary.",
            "long_description": "Containers should be isolated from the host machine as much as possible. The hostPID and hostIPC fields in deployment yaml may allow cross-container influence and may expose the host itself to potentially malicious or destructive actions. This control identifies all pods using hostPID or hostIPC privileges.",
            "controlID": "C-0038",
            "baseScore": 7.0,
            "example": "@controls/examples/c038.yaml",
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Node escape",
                    "id": "Cat-9"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "host-pid-ipc-privileges",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Containers should be as isolated as possible from the host machine. The hostPID and hostIPC fields in Kubernetes may excessively expose the host to potentially malicious actions.",
                    "remediation": "Make sure that the fields hostIPC and hostPID in the pod spec are not set to true (set to false or not present)",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# Fails if pod has hostPID enabled\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tis_host_pid(pod.spec)\n\tpath := \"spec.hostPID\"\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v has hostPID enabled\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\": [],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# Fails if pod has hostIPC enabled\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tis_host_ipc(pod.spec)\n\tpath := \"spec.hostIPC\"\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v has hostIPC enabled\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\": [],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n\n# Fails if workload has hostPID enabled\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tis_host_pid(wl.spec.template.spec)\n\tpath := \"spec.template.spec.hostPID\"\n    msga := {\n\t\"alertMessage\": sprintf(\"%v: %v has a pod with hostPID enabled\", [wl.kind, wl.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if workload has hostIPC enabled\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tis_host_ipc(wl.spec.template.spec)\n\tpath := \"spec.template.spec.hostIPC\"\n    msga := {\n\t\"alertMessage\": sprintf(\"%v: %v has a pod with hostIPC enabled\", [wl.kind, wl.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if cronjob has hostPID enabled\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tis_host_pid(wl.spec.jobTemplate.spec.template.spec)\n\tpath := \"spec.jobTemplate.spec.template.spec.hostPID\"\n    msga := {\n\t\"alertMessage\": sprintf(\"CronJob: %v has a pod with hostPID enabled\", [wl.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if cronjob has hostIPC enabled\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tis_host_ipc(wl.spec.jobTemplate.spec.template.spec)\n\tpath := \"spec.jobTemplate.spec.template.spec.hostIPC\"\n    msga := {\n\t\"alertMessage\": sprintf(\"CronJob: %v has a pod with hostIPC enabled\", [wl.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Check that hostPID and hostIPC are set to false. Default is false. Only in pod spec\n\n\nis_host_pid(podspec){\n    podspec.hostPID == true\n}\n\nis_host_ipc(podspec){\n     podspec.hostIPC == true\n}"
                }
            ]
        },
        {
            "name": "HostNetwork access",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Lateral Movement (Network)"
                        ]
                    }
                ]
            },
            "description": "Potential attackers may gain access to a pod and inherit access to the entire host network. For example, in AWS case, they will have access to the entire VPC. This control identifies all the pods with host network access enabled.",
            "remediation": "Only connect pods to host network when it is necessary. If not, set the hostNetwork field of the pod spec to false, or completely remove it (false is the default). Whitelist only those pods that must have access to host network by design.",
            "long_description": "We have it in ArmoBest",
            "test": "",
            "controlID": "C-0041",
            "baseScore": 7.0,
            "example": "@controls/examples/c041.yaml",
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Network",
                    "id": "Cat-4"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "host-network-access",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if pod has hostNetwork  enabled",
                    "remediation": "Make sure that the hostNetwork field of the pod spec is not set to true (set to false or not present)",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# Fails if pod has hostNetwork enabled\ndeny[msga] {\n    pods := [ pod | pod = input[_] ; pod.kind == \"Pod\"]\n    pod := pods[_]\n\n\tis_host_network(pod.spec)\n\tpath := \"spec.hostNetwork\"\n    msga := {\n\t\"alertMessage\": sprintf(\"Pod: %v is connected to the host network\", [pod.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\":[],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# Fails if workload has hostNetwork enabled\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tis_host_network(wl.spec.template.spec)\n\tpath := \"spec.template.spec.hostNetwork\"\n    msga := {\n\t\"alertMessage\": sprintf(\"%v: %v has a pod connected to the host network\", [wl.kind, wl.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\":[],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if cronjob has hostNetwork enabled\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tis_host_network(wl.spec.jobTemplate.spec.template.spec)\n\tpath := \"spec.jobTemplate.spec.template.spec.hostNetwork\"\n    msga := {\n\t\"alertMessage\": sprintf(\"CronJob: %v has a pod connected to the host network\", [wl.metadata.name]),\n\t\t\"alertScore\": 9,\n\t\t\"deletePaths\": [path],\n\t\t\"failedPaths\": [path],\n\t\t\"fixPaths\":[],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\nis_host_network(podspec) {\n    podspec.hostNetwork == true\n}"
                }
            ]
        },
        {
            "name": "Container hostPort",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "devops"
                ]
            },
            "description": "Configuring hostPort requires a particular port number. If two objects specify the same HostPort, they could not be deployed to the same node. It may prevent the second object from starting, even if Kubernetes will try reschedule it on another node, provided there are available nodes with sufficient amount of resources. Also, if the number of replicas of such workload is higher than the number of nodes, the deployment will consistently fail.",
            "remediation": "Avoid usage of hostPort unless it is absolutely necessary, in which case define appropriate exception. Use NodePort / ClusterIP instead.",
            "long_description": "Workloads (like pod, deployment, etc) that contain a container with hostport. The problem that arises is that if the scale of your workload is larger than the number of nodes in your Kubernetes cluster, the deployment fails. And any two workloads that specify the same HostPort cannot be deployed to the same node. In addition, if the host where your pods are running becomes unavailable, Kubernetes reschedules the pods to different nodes. Thus, if the IP address for your workload changes, external clients of your application will lose access to the pod. The same thing happens when you restart your pods \u2014 Kubernetes reschedules them to a different node if available.\u00a0",
            "test": "Check for each workload (with container) if it exists inside the container hostPort.\u00a0\u00a0",
            "controlID": "C-0044",
            "baseScore": 4.0,
            "example": "@controls/examples/c044.yaml",
            "category": {
                "name": "Network",
                "id": "Cat-4"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "container-hostPort",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container has hostPort",
                    "remediation": "Make sure you do not configure hostPort for the container, if necessary use NodePort / ClusterIP",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# Fails if pod has container with hostPort\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n    container := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n\tpath := is_host_port(container, i, start_of_path)\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v has Host-port\", [ container.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 4,\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# Fails if workload has container with hostPort\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n    path := is_host_port(container, i, start_of_path)\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v in %v: %v   has Host-port\", [ container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 4,\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if cronjob has container with hostPort\ndeny[msga] {\n  \twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n    path := is_host_port(container, i, start_of_path)\n    msga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v in %v: %v   has Host-port\", [ container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 4,\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n\nis_host_port(container, i, start_of_path) = path {\n\tpath = [sprintf(\"%vcontainers[%v].ports[%v].hostPort\", [start_of_path, format_int(i, 10), format_int(j, 10)]) | port = container.ports[j];  port.hostPort]\n\tcount(path) > 0\n}\n"
                }
            ]
        },
        {
            "name": "Writable hostPath mount",
            "attributes": {
                "microsoftMitreColumns": [
                    "Persistence",
                    "Lateral Movement"
                ],
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "devops",
                    "security-impact",
                    "smartRemediation"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Privilege Escalation (Node)"
                        ]
                    }
                ]
            },
            "description": "Mounting host directory to the container can be used by attackers to get access to the underlying host and gain persistence.",
            "remediation": "Refrain from using the hostPath mount or use the exception mechanism to remove unnecessary notifications.",
            "long_description": "hostPath volume mounts a directory or a file from the host to the container. Attackers who have permissions to create a new container in the cluster may create one with a writable hostPath volume and gain persistence on the underlying host. For example, the latter can be achieved by creating a cron job on the host.",
            "test": "Checking in Pod spec if there is a hostPath volume, if it has the section mount.readOnly == false (or doesn\u2019t exist) we raise an alert.",
            "controlID": "C-0045",
            "baseScore": 8.0,
            "example": "@controls/examples/c045.yaml",
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Storage",
                    "id": "Cat-8"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "alert-rw-hostpath",
                    "attributes": {
                        "m$K8sThreatMatrix": "Persistence::Writable hostPath mount, Lateral Movement::Writable volume mounts on the host"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [
                        {
                            "packageName": "cautils"
                        },
                        {
                            "packageName": "kubernetes.api.client"
                        }
                    ],
                    "description": "determines if any workload contains a hostPath volume with rw permissions",
                    "remediation": "Set the readOnly field of the mount to true",
                    "ruleQuery": "",
                    "rule": "package armo_builtins\n\n# Fails if container has a hostPath volume which is not readOnly\n\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n    volumes := pod.spec.volumes\n    volume := volumes[_]\n    volume.hostPath\n\tcontainer := pod.spec.containers[i]\n\tvolume_mount := container.volumeMounts[k]\n\tvolume_mount.name == volume.name\n\tstart_of_path := \"spec.\"\n\tfix_path := is_rw_mount(volume_mount, start_of_path,  i, k)\n\n    podname := pod.metadata.name\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"pod: %v has: %v as hostPath volume\", [podname, volume.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": [fix_path],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# handles majority of workload resources\ndeny[msga] {\n\twl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    volumes := wl.spec.template.spec.volumes\n    volume := volumes[_]\n    volume.hostPath\n\tcontainer := wl.spec.template.spec.containers[i]\n\tvolume_mount := container.volumeMounts[k]\n\tvolume_mount.name == volume.name\n\tstart_of_path := \"spec.template.spec.\"\n\tfix_path := is_rw_mount(volume_mount, start_of_path,  i, k)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v has: %v as hostPath volume\", [wl.kind, wl.metadata.name, volume.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": [fix_path],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\n\t}\n}\n\n# handles CronJobs\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n    volumes := wl.spec.jobTemplate.spec.template.spec.volumes\n    volume := volumes[_]\n    volume.hostPath\n\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tvolume_mount := container.volumeMounts[k]\n\tvolume_mount.name == volume.name\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n\tfix_path := is_rw_mount(volume_mount, start_of_path,  i, k) \n\n\n\tmsga := {\n\t\"alertMessage\": sprintf(\"%v: %v has: %v as hostPath volume\", [wl.kind, wl.metadata.name, volume.name]),\n\t\"packagename\": \"armo_builtins\",\n\t\"alertScore\": 7,\n\t\"fixPaths\": [fix_path],\n\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\nis_rw_mount(mount, start_of_path,  i, k) = fix_path {\n\tnot mount.readOnly == true\n    fix_path = {\"path\": sprintf(\"%vcontainers[%v].volumeMounts[%v].readOnly\", [start_of_path, i, k]), \"value\":\"true\"}\n}\n"
                }
            ]
        },
        {
            "name": "Insecure capabilities",
            "attributes": {
                "actionRequired": "configuration",
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "smartRemediation"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Privilege Escalation (Node)"
                        ]
                    }
                ]
            },
            "description": "Giving insecure or excessive capabilities to a container can increase the impact of the container compromise. This control identifies all the pods with dangerous capabilities (see documentation pages for details).",
            "remediation": "Remove all insecure capabilities which are not necessary for the container.",
            "long_description": "Giving  insecure and unnecessary capabilities for a container can increase the impact of a container compromise.",
            "test": "Check capabilities given against a configurable blacklist of insecure capabilities (https://man7.org/linux/man-pages/man7/capabilities.7.html). ",
            "controlID": "C-0046",
            "baseScore": 7.0,
            "example": "@controls/examples/c046.yaml",
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Node escape",
                    "id": "Cat-9"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "insecure-capabilities",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "configInputs": [
                        "settings.postureControlInputs.insecureCapabilities"
                    ],
                    "controlConfigInputs": [
                        {
                            "path": "settings.postureControlInputs.insecureCapabilities",
                            "name": "Insecure capabilities",
                            "description": "Kubescape looks for these capabilities in containers, which might lead to attackers getting elevated privileges in your cluster. You can see the full list of possible capabilities at https://man7.org/linux/man-pages/man7/capabilities.7.html."
                        }
                    ],
                    "description": "fails if container has insecure capabilities",
                    "remediation": "Remove all insecure capabilities which aren\u2019t necessary for the container.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport data.cautils\n\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n    result := is_dangerous_capabilities(container, start_of_path, i)\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in pod: %v  have dangerous capabilities\", [container.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": result,\n\t\t\"failedPaths\": result,\n\t\t\"fixPaths\": [],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n\tcontainer := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n    result := is_dangerous_capabilities(container, start_of_path, i)\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in workload: %v  have dangerous capabilities\", [container.name, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": result,\n\t\t\"failedPaths\": result,\n\t\t\"fixPaths\": [],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\ndeny[msga] {\n    wl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer := wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n    result := is_dangerous_capabilities(container, start_of_path, i)\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in cronjob: %v  have dangerous capabilities\", [container.name, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": result,\n\t\t\"failedPaths\": result,\n\t\t\"fixPaths\": [],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\nis_dangerous_capabilities(container, start_of_path, i) = path {\n\t# see default-config-inputs.json for list values\n    insecureCapabilities := data.postureControlInputs.insecureCapabilities\n\tpath = [sprintf(\"%vcontainers[%v].securityContext.capabilities.add[%v]\", [start_of_path, format_int(i, 10), format_int(k, 10)]) | capability = container.securityContext.capabilities.add[k]; cautils.list_contains(insecureCapabilities, capability)]\n\tcount(path) > 0\n}"
                }
            ]
        },
        {
            "name": "HostPath mount",
            "attributes": {
                "microsoftMitreColumns": [
                    "Privilege escalation"
                ],
                "controlTypeTags": [
                    "security",
                    "compliance",
                    "smartRemediation"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Privilege Escalation (Node)"
                        ]
                    }
                ]
            },
            "description": "Mounting host directory to the container can be used by attackers to get access to the underlying host. This control identifies all the pods using hostPath mount.",
            "example": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: k8s.gcr.io/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - name: test-volume\n    hostPath: # This field triggers failure!\n      path: /data\n      type: Directory\n",
            "remediation": "Remove hostPath mounts unless they are absolutely necessary and use exception mechanism to remove notifications.",
            "controlID": "C-0048",
            "baseScore": 7.0,
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Storage",
                    "id": "Cat-8"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "alert-any-hostpath",
                    "attributes": {
                        "m$K8sThreatMatrix": "Privilege Escalation::hostPath mount"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "determines if any workload contains a hostPath volume",
                    "remediation": "Try to refrain from using hostPath mounts",
                    "ruleQuery": "",
                    "rule": "package armo_builtins\n\n\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n    volumes := pod.spec.volumes\n    volume := volumes[i]\n\tstart_of_path := \"spec.\"\n\tresult  := is_dangerous_volume(volume, start_of_path, i)\n    podname := pod.metadata.name\n\tvolumeMounts := pod.spec.containers[j].volumeMounts\n\tpathMounts = volume_mounts(volume.name, volumeMounts, sprintf(\"spec.containers[%v]\", [j]))\n\tfinalPath := array.concat([result], pathMounts)\n\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"pod: %v has: %v as hostPath volume\", [podname, volume.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": finalPath,\n\t\t\"failedPaths\": finalPath,\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# handles majority of workload resources\ndeny[msga] {\n\twl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    volumes := wl.spec.template.spec.volumes\n    volume := volumes[i]\n\tstart_of_path := \"spec.template.spec.\"\n    result  := is_dangerous_volume(volume, start_of_path, i)\n\tvolumeMounts := wl.spec.template.spec.containers[j].volumeMounts\n\tpathMounts = volume_mounts(volume.name,volumeMounts, sprintf(\"spec.template.spec.containers[%v]\", [j]))\n\tfinalPath := array.concat([result], pathMounts)\n\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v has: %v as hostPath volume\", [wl.kind, wl.metadata.name, volume.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": finalPath,\n\t\t\"failedPaths\": finalPath,\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# handles CronJobs\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n    volumes := wl.spec.jobTemplate.spec.template.spec.volumes\n    volume := volumes[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n    result  := is_dangerous_volume(volume, start_of_path, i)\n\tvolumeMounts := wl.spec.jobTemplate.spec.template.spec.containers[j].volumeMounts\n\tpathMounts = volume_mounts(volume.name,volumeMounts, sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v]\", [j]))\n\tfinalPath := array.concat([result], pathMounts)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v has: %v as hostPath volume\", [wl.kind, wl.metadata.name, volume.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": finalPath,\n\t\t\"failedPaths\": finalPath,\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\nis_dangerous_volume(volume, start_of_path, i) = path {\n    volume.hostPath.path\n    path = sprintf(\"%vvolumes[%v]\", [start_of_path, format_int(i, 10)])\n}\n\nvolume_mounts(name, volume_mounts, str) = [path] {\n\tname == volume_mounts[j].name\n\tpath := sprintf(\"%s.volumeMounts[%v]\", [str, j])\n} else = []"
                }
            ]
        },
        {
            "name": "Privileged container",
            "attributes": {
                "microsoftMitreColumns": [
                    "Privilege escalation"
                ],
                "controlTypeTags": [
                    "security",
                    "smartRemediation"
                ]
            },
            "description": "Potential attackers may gain access to privileged containers and inherit access to the host resources. Therefore, it is not recommended to deploy privileged containers unless it is absolutely necessary. This control identifies all the privileged Pods.",
            "example": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  containers:\n    - name: pause\n      image: k8s.gcr.io/pause\n      securityContext:\n          privileged: true # This field triggers failure!\n",
            "remediation": "Remove privileged capabilities by setting the securityContext.privileged to false. If you must deploy a Pod as privileged, add other restriction to it, such as network policy, Seccomp etc and still remove all unnecessary capabilities. Use the exception mechanism to remove unnecessary notifications.",
            "long_description": "A privileged container is a container that has all the capabilities of the host machine, which lifts all the limitations regular containers have. Practically, this means that privileged containers can do almost every action that can be performed directly on the host. Attackers who gain access to a privileged container or have permissions to create a new privileged container (by using the compromised pod\u2019s service account, for example), can get access to the host\u2019s resources.",
            "test": "Check in Pod spec if securityContext.privileged == true, if so raise an alert.",
            "controlID": "C-0057",
            "baseScore": 8.0,
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Node escape",
                    "id": "Cat-9"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "rule-privilege-escalation",
                    "attributes": {
                        "m$K8sThreatMatrix": "Privilege Escalation::privileged container",
                        "mitre": "Privilege Escalation",
                        "mitreCode": "TA0004"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "determines if pods/deployments defined as privileged true",
                    "remediation": "avoid defining pods as privilleged",
                    "ruleQuery": "",
                    "rule": "package armo_builtins\n# Deny mutating action unless user is in group owning the resource\n\n\n# privileged pods\ndeny[msga] {\n\n\tpod := input[_]\n\tpod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n\tpath := isPrivilegedContainer(container, i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"the following pods are defined as privileged: %v\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 3,\n\t\t\"fixPaths\": [],\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n         \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n     }\n}\n\n\n# handles majority of workload resources\ndeny[msga] {\n\twl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n\tcontainer := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n\tpath := isPrivilegedContainer(container, i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v is defined as privileged:\", [wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 3,\n\t\t\"fixPaths\": [],\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n         \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n     }\n}\n\n# handles cronjob\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer := wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n\tpath := isPrivilegedContainer(container, i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"the following cronjobs are defined as privileged: %v\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 3,\n\t\t\"fixPaths\": [],\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n         \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n     }\n}\n\n\n# Only SYS_ADMIN capabilite\nisPrivilegedContainer(container, i, start_of_path) = path {\n\tnot container.securityContext.privileged == true\n\tpath = [sprintf(\"%vcontainers[%v].securityContext.capabilities.add[%v]\", [start_of_path, format_int(i, 10), format_int(k, 10)]) | capabilite = container.securityContext.capabilities.add[k]; capabilite == \"SYS_ADMIN\"]\n\tcount(path) > 0\n}\n\n# Only securityContext.privileged == true\nisPrivilegedContainer(container, i, start_of_path) = path {\n\tcontainer.securityContext.privileged == true\n\tpath1 = [sprintf(\"%vcontainers[%v].securityContext.capabilities.add[%v]\", [start_of_path, format_int(i, 10), format_int(k, 10)]) | capabilite = container.securityContext.capabilities.add[k]; capabilite == \"SYS_ADMIN\"]\n\tcount(path1) < 1\n\tpath = [sprintf(\"%vcontainers[%v].securityContext.privileged\", [start_of_path, format_int(i, 10)])]\n}\n\n# SYS_ADMIN capabilite && securityContext.privileged == true\nisPrivilegedContainer(container, i, start_of_path) = path {\n\tpath1 = [sprintf(\"%vcontainers[%v].securityContext.capabilities.add[%v]\", [start_of_path, format_int(i, 10), format_int(k, 10)]) | capabilite = container.securityContext.capabilities.add[k]; capabilite == \"SYS_ADMIN\"]\n\tcount(path1) > 0\n\tcontainer.securityContext.privileged == true\n\tpath = array.concat(path1, [sprintf(\"%vcontainers[%v].securityContext.privileged\", [start_of_path, format_int(i, 10)])])\n}"
                }
            ]
        },
        {
            "name": "Secret/etcd encryption enabled",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "All Kubernetes Secrets are stored primarily in etcd therefore it is important to encrypt it.",
            "remediation": "Turn on the etcd encryption in your cluster, for more see the vendor documentation.",
            "long_description": "etcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data. All object data in Kubernetes, like secrets, are stored there. This is the reason why it is important to protect the contents of etcd and use its data encryption feature.",
            "test": "Reading the cluster description from the managed cloud API (EKS, GKE), or the API server pod configuration for native K8s and checking if etcd encryption is enabled",
            "controlID": "C-0066",
            "baseScore": 6.0,
            "category": {
                "name": "Control plane",
                "id": "Cat-1"
            },
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "secret-etcd-encryption-cloud",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [],
                            "apiVersions": [],
                            "resources": []
                        }
                    ],
                    "dynamicMatch": [
                        {
                            "apiGroups": [
                                "management.azure.com"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "ClusterDescribe"
                            ]
                        },
                        {
                            "apiGroups": [
                                "eks.amazonaws.com"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "ClusterDescribe"
                            ]
                        },
                        {
                            "apiGroups": [
                                "container.googleapis.com"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "ClusterDescribe"
                            ]
                        }
                    ],
                    "relevantCloudProviders": [
                        "AKS",
                        "EKS",
                        "GKE"
                    ],
                    "ruleDependencies": [],
                    "description": "",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# Check if encryption in etcd in enabled for AKS\ndeny[msga] {\n\tcluster_config := input[_]\n\tcluster_config.apiVersion == \"management.azure.com/v1\"\n\tcluster_config.kind == \"ClusterDescribe\"\n    cluster_config.metadata.provider == \"aks\"\t\n\tconfig = cluster_config.data\n\n\tnot isEncryptedAKS(config)\n\t\n\tmsga := {\n\t\t\"alertMessage\": \"etcd/secret encryption is not enabled\",\n\t\t\"alertScore\": 3,\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [],\n\t\t\"fixCommand\": \"az aks nodepool add --name hostencrypt --cluster-name <myAKSCluster> --resource-group <myResourceGroup> -s Standard_DS2_v2 -l <myRegion> --enable-encryption-at-host\",\n\t\t\"alertObject\": {\n            \"externalObjects\": cluster_config\n\t\t}\n\t}\n}\n\n\n# Check if encryption in etcd in enabled for EKS\ndeny[msga] {\n\tcluster_config := input[_]\n\tcluster_config.apiVersion == \"eks.amazonaws.com/v1\"\n\tcluster_config.kind == \"ClusterDescribe\"\n    cluster_config.metadata.provider == \"eks\"\t\n\tconfig = cluster_config.data\n\n\tis_not_encrypted_EKS(config)\n    \n\t\n\tmsga := {\n\t\t\"alertMessage\": \"etcd/secret encryption is not enabled\",\n\t\t\"alertScore\": 3,\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [],\n\t\t\"fixCommand\": \"eksctl utils enable-secrets-encryption --cluster=<cluster> --key-arn=arn:aws:kms:<cluster_region>:<account>:key/<key> --region=<region>\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [],\n            \"externalObjects\": cluster_config\n\t\t}\n\t}\n}\n\n\n\n# Check if encryption in etcd in enabled for GKE\ndeny[msga] {\n\tcluster_config := input[_]\n\tcluster_config.apiVersion == \"container.googleapis.com/v1\"\n\tcluster_config.kind == \"ClusterDescribe\"\n    cluster_config.metadata.provider == \"gke\"\t\n\tconfig := cluster_config.data\n\n\tnot is_encrypted_GKE(config)\n    \n\t\n\tmsga := {\n\t\t\"alertMessage\": \"etcd/secret encryption is not enabled\",\n\t\t\"alertScore\": 3,\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"reviewPaths\": [\"data.database_encryption.state\"],\n\t\t\"failedPaths\": [\"data.database_encryption.state\"],\n\t\t\"fixPaths\": [],\n\t\t\"fixCommand\": \"gcloud container clusters update <cluster_name> --region=<compute_region> --database-encryption-key=<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name> --project=<cluster_project_id>\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [],\n            \"externalObjects\": cluster_config\n\t\t}\n\t}\n}\n\nis_encrypted_GKE(config) {\n\t config.database_encryption.state == \"1\"\n}\nis_encrypted_GKE(config) {\n\t config.database_encryption.state == \"ENCRYPTED\"\n}\n\nis_not_encrypted_EKS(cluster_config) {\n\tencryptionConfig := cluster_config.Cluster.EncryptionConfig[_]\n    goodResources := [resource  | resource =   cluster_config.Cluster.EncryptionConfig.Resources[_]; resource == \"secrets\"]\n\tcount(goodResources) == 0\n}\n\nis_not_encrypted_EKS(cluster_config) {\n\tcluster_config.Cluster.EncryptionConfig == null\n}\n\nis_not_encrypted_EKS(cluster_config) {\n\tcount(cluster_config.Cluster.EncryptionConfig) == 0\n}\n\nis_not_encrypted_EKS(cluster_config) {\n\tencryptionConfig := cluster_config.Cluster.EncryptionConfig[_]\n    count(encryptionConfig.Resources) == 0\n}\n\nisEncryptedAKS(cluster_config) {\n\tcluster_config.properties.agentPoolProfiles.enableEncryptionAtHost == true\n}\n"
                },
                {
                    "name": "etcd-encryption-native",
                    "attributes": {
                        "resourcesAggregator": "apiserver-pod",
                        "useFromKubescapeVersion": "v1.0.133"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport data.cautils\n\n# Check if encryption in etcd is enabled for native k8s\ndeny[msga] {\n\tapiserverpod := input[_]\n\tcmd := apiserverpod.spec.containers[0].command\n\tenc_command := [command | command := cmd[_]; contains(command, \"--encryption-provider-config=\")]\n\tcount(enc_command) < 1\n\tfixpath := {\"path\":sprintf(\"spec.containers[0].command[%d]\", [count(cmd)]), \"value\": \"--encryption-provider-config=YOUR_VALUE\"}\n\n\tmsga := {\n\t\t\"alertMessage\": \"etcd encryption is not enabled\",\n\t\t\"alertScore\": 9,\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [fixpath],\n\t\t\"alertObject\": {\"k8sApiObjects\": [apiserverpod]},\n\t}\n}\n"
                }
            ]
        },
        {
            "name": "Disable anonymous access to Kubelet service",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "By default, requests to the kubelet's HTTPS endpoint that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of system:anonymous and a group of system:unauthenticated.",
            "remediation": "Start the kubelet with the --anonymous-auth=false flag.",
            "long_description": "By default, requests to the kubelet's HTTPS endpoint that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of system:anonymous and a group of system:unauthenticated.",
            "test": "Reading the kubelet command lines and configuration file looking for anonymous-auth configuration. If this configuration is set on both, the command line values take precedence over it.",
            "controlID": "C-0069",
            "category": {
                "name": "Control plane",
                "id": "Cat-1"
            },
            "baseScore": 10.0,
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "anonymous-requests-to-kubelet-service-updated",
                    "attributes": {
                        "hostSensorRule": "true"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [],
                            "apiVersions": [],
                            "resources": []
                        }
                    ],
                    "dynamicMatch": [
                        {
                            "apiGroups": [
                                "hostdata.kubescape.cloud"
                            ],
                            "apiVersions": [
                                "v1beta0"
                            ],
                            "resources": [
                                "KubeletInfo"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Determines if anonymous requests to the kubelet service are allowed.",
                    "remediation": "Disable anonymous requests by setting  the anonymous-auth flag to false, or using the kubelet configuration file.",
                    "ruleQuery": "",
                    "rule": "package armo_builtins\n\n# CIS 4.2.1 https://workbench.cisecurity.org/sections/1126668/recommendations/1838638\n\ndeny[msga] {\n\tobj := input[_]\n\tis_kubelet_info(obj)\n\tcommand := obj.data.cmdLine\n\n\tcontains(command, \"--anonymous-auth\")\n\tcontains(command, \"--anonymous-auth=true\")\n\n\texternal_obj := json.filter(obj, [\"apiVersion\", \"data/cmdLine\", \"kind\", \"metadata\"])\n\n\tmsga := {\n\t\t\"alertMessage\": \"Anonymous requests is enabled.\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\"externalObjects\": external_obj},\n\t}\n}\n\n\ndeny[msga] {\n\tobj := input[_]\n\tis_kubelet_info(obj)\n\tcommand := obj.data.cmdLine\n\n\tnot contains(command, \"--anonymous-auth\")\n\tcontains(command, \"--config\")\n\n\tdecodedConfigContent := base64.decode(obj.data.configFile.content)\n\tyamlConfig := yaml.unmarshal(decodedConfigContent)\n\tyamlConfig.authentication.anonymous.enabled == true\n\n\tmsga := {\n\t\t\"alertMessage\": \"Anonymous requests is enabled.\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [\"authentication.anonymous.enabled\"],\n\t\t\"failedPaths\": [\"authentication.anonymous.enabled\"],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\"externalObjects\": {\n\t\t\t\"apiVersion\": obj.apiVersion,\n\t\t\t\"kind\": obj.kind,\n\t\t\t\"metadata\": obj.metadata,\n\t\t\t\"data\": {\"configFile\": {\"content\": decodedConfigContent}},\n\t\t}},\n\t}\n}\n\n## Host sensor failed to get config file content\ndeny[msga] {\n\tobj := input[_]\n\tis_kubelet_info(obj)\n\n\tcommand := obj.data.cmdLine\n\n\tnot contains(command, \"--anonymous-auth\")\n\tcontains(command, \"--config\")\n\n\tnot obj.data.configFile.content\n\n\tmsga := {\n\t\t\"alertMessage\": \"Failed to analyze config file\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\"externalObjects\": {\n\t\t\t\"apiVersion\": obj.apiVersion,\n\t\t\t\"kind\": obj.kind,\n\t\t\t\"data\": obj.data,\n\t\t}},\n\t}\n}\n\nis_kubelet_info(obj) {\n\tobj.kind == \"KubeletInfo\"\n\tobj.apiVersion == \"hostdata.kubescape.cloud/v1beta0\"\n}\n"
                }
            ]
        },
        {
            "name": "Enforce Kubelet client TLS authentication",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "Kubelets are the node level orchestrator in Kubernetes control plane. They are publishing service port 10250 where they accept commands from API server. Operator must make sure that only API server is allowed to submit commands to Kubelet. This is done through client certificate verification, must configure Kubelet with client CA file to use for this purpose.",
            "remediation": "Start the kubelet with the --client-ca-file flag, providing a CA bundle to verify client certificates with.",
            "long_description": "Kubelets are the node level orchestrator in Kubernetes control plane. They are publishing service port 10250 where they accept commands from API server. Operator must make sure that only API server is allowed to submit commands to Kubelet. This is done through client certificate verification, must configure Kubelet with client CA file to use for this purpose.",
            "test": "Reading the kubelet command lines and configuration file looking for client TLS configuration.",
            "controlID": "C-0070",
            "baseScore": 9.0,
            "category": {
                "name": "Control plane",
                "id": "Cat-1"
            },
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "enforce-kubelet-client-tls-authentication-updated",
                    "attributes": {
                        "hostSensorRule": "true"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [],
                            "apiVersions": [],
                            "resources": []
                        }
                    ],
                    "dynamicMatch": [
                        {
                            "apiGroups": [
                                "hostdata.kubescape.cloud"
                            ],
                            "apiVersions": [
                                "v1beta0"
                            ],
                            "resources": [
                                "KubeletInfo"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Determines if kubelet client tls authentication is enabled.",
                    "remediation": "Start the kubelet with the --client-ca-file flag, providing a CA bundle to verify client certificates with.",
                    "ruleQuery": "",
                    "rule": "package armo_builtins\n\n# CIS 4.2.3 https://workbench.cisecurity.org/sections/1126668/recommendations/1838643\n\ndeny[msga] {\n\tobj := input[_]\n\tis_kubelet_info(obj)\n\n\tcommand := obj.data.cmdLine\n\n\tnot contains(command, \"--client-ca-file\")\n\tcontains(command, \"--config\")\n\n\tdecodedConfigContent := base64.decode(obj.data.configFile.content)\n\tyamlConfig := yaml.unmarshal(decodedConfigContent)\n\tnot yamlConfig.authentication.x509.clientCAFile\n\n\tmsga := {\n\t\t\"alertMessage\": \"kubelet client TLS authentication is not enabled\",\n\t\t\"alertScore\": 6,\n\t\t\"reviewPaths\": [\"authentication.x509.clientCAFile\"],\n\t\t\"failedPaths\": [\"authentication.x509.clientCAFile\"],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\"externalObjects\": {\n\t\t\t\"apiVersion\": obj.apiVersion,\n\t\t\t\"kind\": obj.kind,\n\t\t\t\"metadata\": obj.metadata,\n\t\t\t\"data\": {\"configFile\": {\"content\": decodedConfigContent}},\n\t\t}},\n\t}\n}\n\ndeny[msga] {\n\tobj := input[_]\n\tis_kubelet_info(obj)\n\n\tcommand := obj.data.cmdLine\n\n\tnot contains(command, \"--client-ca-file\")\n\tnot contains(command, \"--config\")\n\n\tmsga := {\n\t\t\"alertMessage\": \"kubelet client TLS authentication is not enabled\",\n\t\t\"alertScore\": 6,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\"externalObjects\": {\n\t\t\t\"apiVersion\": obj.apiVersion,\n\t\t\t\"kind\": obj.kind,\n\t\t\t\"data\": {\"cmdLine\": command},\n\t\t}},\n\t}\n}\n\n## Host sensor failed to get config file content\ndeny[msga] {\n\tobj := input[_]\n\tis_kubelet_info(obj)\n\n\tcommand := obj.data.cmdLine\n\n\tnot contains(command, \"--client-ca-file\")\n\tcontains(command, \"--config\")\n\n\tnot obj.data.configFile.content\n\n\tmsga := {\n\t\t\"alertMessage\": \"Failed to analyze config file\",\n\t\t\"alertScore\": 6,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\"externalObjects\": {\n\t\t\t\"apiVersion\": obj.apiVersion,\n\t\t\t\"kind\": obj.kind,\n\t\t\t\"data\": obj.data,\n\t\t}},\n\t}\n}\n\nis_kubelet_info(obj) {\n\tobj.kind == \"KubeletInfo\"\n\tobj.apiVersion == \"hostdata.kubescape.cloud/v1beta0\"\n}\n"
                }
            ]
        },
        {
            "name": "Container runtime socket mounted",
            "attributes": {
                "controlTypeTags": [
                    "devops",
                    "smartRemediation"
                ]
            },
            "description": "Mounting Container runtime socket (Unix socket) enables container to access Container runtime, retrieve sensitive information and execute commands, if Container runtime is available. This control identifies pods that attempt to mount Container runtime socket for accessing Container runtime.",
            "remediation": "Remove container runtime socket mount request or define an exception.",
            "long_description": "Mounting Docker socket (Unix socket) enables container to access Docker internals, retrieve sensitive information and execute Docker commands, if Docker runtime is available. This control identifies pods that attempt to mount Docker socket for accessing Docker runtime.",
            "test": "Check hostpath. If the path is set to one of the container runtime socket, the container has access to container runtime - fail.",
            "controlID": "C-0074",
            "baseScore": 5.0,
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "containers-mounting-docker-socket",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Check hostpath. If the path is set to one of the container runtime socket, the container has access to container runtime - fail.",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n    volume := pod.spec.volumes[i]\n\thost_path := volume.hostPath\n    is_runtime_socket_mounting(host_path)\n\tpath := sprintf(\"spec.volumes[%v]\", [format_int(i, 10)])\n\tvolumeMounts := pod.spec.containers[j].volumeMounts\n\tpathMounts = volume_mounts(volume.name, volumeMounts, sprintf(\"spec.containers[%v]\", [j]))\n\tfinalPath := array.concat([path], pathMounts)\n    msga := {\n\t\t\"alertMessage\": sprintf(\"volume: %v in pod: %v has mounting to Docker internals.\", [volume.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"deletePaths\":finalPath,\n\t\t\"failedPaths\": finalPath,\n\t\t\"fixPaths\":[],\n\t\t\"alertScore\": 5,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\t\n}\n\n\n\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    volume := wl.spec.template.spec.volumes[i]\n\thost_path := volume.hostPath\n    is_runtime_socket_mounting(host_path)\n\tpath := sprintf(\"spec.template.spec.volumes[%v]\", [format_int(i, 10)])\n\tvolumeMounts := wl.spec.template.spec.containers[j].volumeMounts\n\tpathMounts = volume_mounts(volume.name,volumeMounts, sprintf(\"spec.template.spec.containers[%v]\", [j]))\n\tfinalPath := array.concat([path], pathMounts)\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"volume: %v in %v: %v has mounting to Docker internals.\", [ volume.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"deletePaths\": finalPath,\n\t\t\"failedPaths\": finalPath,\n\t\t\"fixPaths\":[],\n\t\t\"alertScore\": 5,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\ndeny[msga] {\n  \twl := input[_]\n\twl.kind == \"CronJob\"\n\tvolume = wl.spec.jobTemplate.spec.template.spec.volumes[i]\n    host_path := volume.hostPath\n    is_runtime_socket_mounting(host_path)\n\tpath := sprintf(\"spec.jobTemplate.spec.template.spec.volumes[%v]\", [format_int(i, 10)])\n\tvolumeMounts := wl.spec.jobTemplate.spec.template.spec.containers[j].volumeMounts\n\tpathMounts = volume_mounts(volume.name,volumeMounts, sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v]\", [j]))\n\tfinalPath := array.concat([path], pathMounts)\n    msga := {\n\t\t\"alertMessage\": sprintf(\"volume: %v in %v: %v has mounting to Docker internals.\", [ volume.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"deletePaths\": finalPath,\n\t\t\"failedPaths\": finalPath,\n\t\t\"fixPaths\":[],\n\t\t\"alertScore\": 5,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\nvolume_mounts(name, volume_mounts, str) = [path] {\n\tname == volume_mounts[j].name\n\tpath := sprintf(\"%s.volumeMounts[%v]\", [str, j])\n} else = []\n\nis_runtime_socket_mounting(host_path) {\n\thost_path.path == \"/var/run/docker.sock\"\n}\n\nis_runtime_socket_mounting(host_path) {\n\thost_path.path == \"/var/run/docker\"\n}\n\nis_runtime_socket_mounting(host_path) {\n\thost_path.path == \"/run/containerd/containerd.sock\"\n}\n\nis_runtime_socket_mounting(host_path) {\n\thost_path.path == \"/var/run/crio/crio.sock\"\n}\n"
                }
            ]
        },
        {
            "name": "Apply Security Context to Your Pods and Containers",
            "controlID": "C-0211",
            "description": "Apply Security Context to Your Pods and Containers",
            "long_description": "A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context.",
            "remediation": "Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers.",
            "test": "Check that pod and container security context fields according to recommendations in CIS Security Benchmark for Docker Containers",
            "manual_test": "Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.",
            "references": [
                "https://workbench.cisecurity.org/sections/1126667/recommendations/1838636"
            ],
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Privilege Escalation (Node)"
                        ]
                    }
                ]
            },
            "baseScore": 8,
            "impact_statement": "If you incorrectly apply security contexts, you may have trouble running the pods.",
            "default_value": "By default, no security contexts are automatically applied to pods.",
            "category": {
                "name": "Workload",
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "rule-privilege-escalation",
                    "attributes": {
                        "m$K8sThreatMatrix": "Privilege Escalation::privileged container",
                        "mitre": "Privilege Escalation",
                        "mitreCode": "TA0004"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "determines if pods/deployments defined as privileged true",
                    "remediation": "avoid defining pods as privilleged",
                    "ruleQuery": "",
                    "rule": "package armo_builtins\n# Deny mutating action unless user is in group owning the resource\n\n\n# privileged pods\ndeny[msga] {\n\n\tpod := input[_]\n\tpod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n\tpath := isPrivilegedContainer(container, i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"the following pods are defined as privileged: %v\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 3,\n\t\t\"fixPaths\": [],\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n         \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n     }\n}\n\n\n# handles majority of workload resources\ndeny[msga] {\n\twl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n\tcontainer := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n\tpath := isPrivilegedContainer(container, i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v is defined as privileged:\", [wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 3,\n\t\t\"fixPaths\": [],\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n         \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n     }\n}\n\n# handles cronjob\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer := wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n\tpath := isPrivilegedContainer(container, i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"the following cronjobs are defined as privileged: %v\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 3,\n\t\t\"fixPaths\": [],\n\t\t\"deletePaths\": path,\n\t\t\"failedPaths\": path,\n         \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n     }\n}\n\n\n# Only SYS_ADMIN capabilite\nisPrivilegedContainer(container, i, start_of_path) = path {\n\tnot container.securityContext.privileged == true\n\tpath = [sprintf(\"%vcontainers[%v].securityContext.capabilities.add[%v]\", [start_of_path, format_int(i, 10), format_int(k, 10)]) | capabilite = container.securityContext.capabilities.add[k]; capabilite == \"SYS_ADMIN\"]\n\tcount(path) > 0\n}\n\n# Only securityContext.privileged == true\nisPrivilegedContainer(container, i, start_of_path) = path {\n\tcontainer.securityContext.privileged == true\n\tpath1 = [sprintf(\"%vcontainers[%v].securityContext.capabilities.add[%v]\", [start_of_path, format_int(i, 10), format_int(k, 10)]) | capabilite = container.securityContext.capabilities.add[k]; capabilite == \"SYS_ADMIN\"]\n\tcount(path1) < 1\n\tpath = [sprintf(\"%vcontainers[%v].securityContext.privileged\", [start_of_path, format_int(i, 10)])]\n}\n\n# SYS_ADMIN capabilite && securityContext.privileged == true\nisPrivilegedContainer(container, i, start_of_path) = path {\n\tpath1 = [sprintf(\"%vcontainers[%v].securityContext.capabilities.add[%v]\", [start_of_path, format_int(i, 10), format_int(k, 10)]) | capabilite = container.securityContext.capabilities.add[k]; capabilite == \"SYS_ADMIN\"]\n\tcount(path1) > 0\n\tcontainer.securityContext.privileged == true\n\tpath = array.concat(path1, [sprintf(\"%vcontainers[%v].securityContext.privileged\", [start_of_path, format_int(i, 10)])])\n}"
                },
                {
                    "name": "immutable-container-filesystem",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container has mutable filesystem",
                    "remediation": "Make sure that the securityContext.readOnlyRootFilesystem field in the container/pod spec is set to true",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# Fails if pods has container with mutable filesystem\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n    is_mutable_filesystem(container)\n\tfixPath = {\"path\": sprintf(\"%vcontainers[%d].securityContext.readOnlyRootFilesystem\", [start_of_path, i]), \"value\": \"true\"}\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in pod: %v  has  mutable filesystem\", [container.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [fixPath],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# Fails if workload has  container with mutable filesystem \ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n    is_mutable_filesystem(container)\n\tfixPath = {\"path\": sprintf(\"%vcontainers[%d].securityContext.readOnlyRootFilesystem\", [start_of_path, i]), \"value\": \"true\"}\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v has  mutable filesystem\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [fixPath],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if cronjob has  container with mutable filesystem \ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n\tis_mutable_filesystem(container)\n\tfixPath = {\"path\": sprintf(\"%vcontainers[%d].securityContext.readOnlyRootFilesystem\", [start_of_path, i]), \"value\": \"true\"}\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v has mutable filesystem\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [fixPath],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Default of readOnlyRootFilesystem is false. This field is only in container spec and not pod spec\nis_mutable_filesystem(container) {\n\tcontainer.securityContext.readOnlyRootFilesystem == false\n}\n\nis_mutable_filesystem(container) {\n\tnot container.securityContext.readOnlyRootFilesystem == false\n    not container.securityContext.readOnlyRootFilesystem == true\n}\n"
                },
                {
                    "name": "non-root-containers",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container can run as root",
                    "remediation": "Make sure that the user/group in the securityContext of pod/container is set to an id over 0, or the runAsNonRoot flag is set to true.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n################################################################################\n# Rules\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\n\tstart_of_path := \"spec\"\n\trun_as_user_fixpath := evaluate_workload_run_as_user(container, pod, start_of_path)\n\trun_as_group_fixpath := evaluate_workload_run_as_group(container, pod, start_of_path)\n\tall_fixpaths := array.concat(run_as_user_fixpath, run_as_group_fixpath)\n\tcount(all_fixpaths) > 0\n\tfixPaths := get_fixed_paths(all_fixpaths, i)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in pod: %v  may run as root\", [container.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n        \"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n\n\tstart_of_path := \"spec.template.spec\"\n\trun_as_user_fixpath := evaluate_workload_run_as_user(container, wl.spec.template, start_of_path)\n\trun_as_group_fixpath := evaluate_workload_run_as_group(container, wl.spec.template, start_of_path)\n\tall_fixpaths := array.concat(run_as_user_fixpath, run_as_group_fixpath)\n\tcount(all_fixpaths) > 0\n\tfixPaths := get_fixed_paths(all_fixpaths, i)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in %v: %v may run as root\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n        \"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if cronjob has a container configured to run as root\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec\"\n\trun_as_user_fixpath := evaluate_workload_run_as_user(container, wl.spec.jobTemplate.spec.template, start_of_path)\n\trun_as_group_fixpath := evaluate_workload_run_as_group(container, wl.spec.jobTemplate.spec.template, start_of_path)\n\tall_fixpaths := array.concat(run_as_user_fixpath, run_as_group_fixpath)\n\tcount(all_fixpaths) > 0\n\tfixPaths := get_fixed_paths(all_fixpaths, i)\n\t\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in %v: %v  may run as root\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n        \"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\nget_fixed_paths(all_fixpaths, i) = [{\"path\":replace(all_fixpaths[0].path,\"container_ndx\",format_int(i,10)), \"value\":all_fixpaths[0].value}, {\"path\":replace(all_fixpaths[1].path,\"container_ndx\",format_int(i,10)), \"value\":all_fixpaths[1].value}]{\n\tcount(all_fixpaths) == 2\n} else = [{\"path\":replace(all_fixpaths[0].path,\"container_ndx\",format_int(i,10)), \"value\":all_fixpaths[0].value}] \n\n#################################################################################\n# Workload evaluation \n\n# if runAsUser is set to 0 and runAsNonRoot is set to false/ not set - suggest to set runAsUser to 1000\n# if runAsUser is not set and runAsNonRoot is set to false/ not set - suggest to set runAsNonRoot to true\n# all checks are both on the pod and the container level\nevaluate_workload_run_as_user(container, pod, start_of_path) = fixPath {\n\trunAsNonRootValue := get_run_as_non_root_value(container, pod, start_of_path)\n\trunAsNonRootValue.value == false\n\t\n\trunAsUserValue := get_run_as_user_value(container, pod, start_of_path)\n\trunAsUserValue.value == 0\n\n\talertInfo := choose_first_if_defined(runAsUserValue, runAsNonRootValue)\n\tfixPath := alertInfo.fixPath\n} else = [] \n\n\n# if runAsGroup is set to 0/ not set - suggest to set runAsGroup to 1000\n# all checks are both on the pod and the container level\nevaluate_workload_run_as_group(container, pod, start_of_path) = fixPath {\t\n\trunAsGroupValue := get_run_as_group_value(container, pod, start_of_path)\n\trunAsGroupValue.value == 0\n\n\tfixPath := runAsGroupValue.fixPath\n} else = []\n\n\n#################################################################################\n# Value resolution functions\n\n\nget_run_as_non_root_value(container, pod, start_of_path) = runAsNonRoot {\n    runAsNonRoot := {\"value\" : container.securityContext.runAsNonRoot, \"fixPath\": [{\"path\": sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]), \"value\":\"true\"}], \"defined\" : true}\n} else = runAsNonRoot {\n    runAsNonRoot := {\"value\" : pod.spec.securityContext.runAsNonRoot, \"fixPath\": [{\"path\": sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]), \"value\":\"true\"}], \"defined\" : true}\n}  else = {\"value\" : false, \"fixPath\": [{\"path\":  sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]) , \"value\":\"true\"}], \"defined\" : false}\n\nget_run_as_user_value(container, pod, start_of_path) = runAsUser {\n\tpath := sprintf(\"%v.containers[container_ndx].securityContext.runAsUser\", [start_of_path]) \n    runAsUser := {\"value\" : container.securityContext.runAsUser, \"fixPath\": [{\"path\": path, \"value\": \"1000\"}], \"defined\" : true}\n} else = runAsUser {\n\tpath := sprintf(\"%v.securityContext.runAsUser\", [start_of_path]) \n    runAsUser := {\"value\" : pod.spec.securityContext.runAsUser, \"fixPath\": [{\"path\": path, \"value\": \"1000\"}],\"defined\" : true}\n} else = {\"value\" : 0, \"fixPath\": [{\"path\":  sprintf(\"%v.containers[container_ndx].securityContext.runAsNonRoot\", [start_of_path]), \"value\":\"true\"}],\n\t\"defined\" : false}\n\nget_run_as_group_value(container, pod, start_of_path) = runAsGroup {\n\tpath := sprintf(\"%v.containers[container_ndx].securityContext.runAsGroup\", [start_of_path])\n    runAsGroup := {\"value\" : container.securityContext.runAsGroup, \"fixPath\": [{\"path\": path, \"value\": \"1000\"}],\"defined\" : true}\n} else = runAsGroup {\n\tpath := sprintf(\"%v.securityContext.runAsGroup\", [start_of_path])\n    runAsGroup := {\"value\" : pod.spec.securityContext.runAsGroup, \"fixPath\":[{\"path\": path, \"value\": \"1000\"}], \"defined\" : true}\n} else = {\"value\" : 0, \"fixPath\": [{\"path\": sprintf(\"%v.containers[container_ndx].securityContext.runAsGroup\", [start_of_path]), \"value\":\"1000\"}],\n \t\"defined\" : false\n}\n\nchoose_first_if_defined(l1, l2) = c {\n    l1.defined\n    c := l1\n} else = l2\n\n"
                },
                {
                    "name": "drop-capability-netraw",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container does not drop the capability NET_RAW",
                    "remediation": "Define the drop list in security context capabilities to include NET_RAW.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.in\n\n# Fails if pod does not drop the capability NET_RAW \ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"Pod\"\n\tpath_to_containers := [\"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n\n\tpath_to_search := [\"securityContext\", \"capabilities\"]\n\tresult := container_doesnt_drop_NET_RAW(container, i, path_to_containers, path_to_search)\n\tfailedPaths := get_failed_path(result)\n    fixPaths := get_fixed_path(result)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %s does not drop the capability NET_RAW\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": failedPaths,\n\t\t\"failedPaths\": failedPaths,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\n# Fails if workload does not drop the capability NET_RAW\ndeny[msga] {\n\twl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n\tpath_to_containers := [\"spec\", \"template\", \"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n\n\tpath_to_search := [\"securityContext\", \"capabilities\"]\n\tresult := container_doesnt_drop_NET_RAW(container, i, path_to_containers, path_to_search)\n\tfailedPaths := get_failed_path(result)\n    fixPaths := get_fixed_path(result)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v does not drop the capability NET_RAW\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": failedPaths,\n\t\t\"failedPaths\": failedPaths,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\n# Fails if CronJob does not drop the capability NET_RAW\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tpath_to_containers := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n\n\tpath_to_search := [\"securityContext\", \"capabilities\"]\n\tresult := container_doesnt_drop_NET_RAW(container, i, path_to_containers, path_to_search)\n\tfailedPaths := get_failed_path(result)\n    fixPaths := get_fixed_path(result)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Cronjob: %v does not drop the capability NET_RAW\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"deletePaths\": failedPaths,\n\t\t\"failedPaths\": failedPaths,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\n# Checks if workload does not drop the capability NET_RAW\ncontainer_doesnt_drop_NET_RAW(container, i, path_to_containers, path_to_search) = [failed_path, fix_path] {\n\tpath_to_drop := array.concat(path_to_search, [\"drop\"])\n\tdrop_list := object.get(container, path_to_drop, [])\n\tnot \"NET_RAW\" in drop_list\n\tnot \"ALL\" in drop_list\n\tnot \"all\" in drop_list\n\tfixpath := sprintf(\"%s[%d].%s[%d]\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_drop), count(drop_list)])\n\tfix_path := [{\"path\": fixpath, \"value\": \"NET_RAW\"}]\n\tfailed_path := \"\"\n}\n\n# Checks if workload drops all capabilities but adds NET_RAW capability\ncontainer_doesnt_drop_NET_RAW(container, i, path_to_containers, path_to_search) = [failed_path, fix_path] {\n\tpath_to_drop := array.concat(path_to_search, [\"drop\"])\n\tdrop_list := object.get(container, path_to_drop, [])\n\tall_in_list(drop_list)\n\tpath_to_add := array.concat(path_to_search, [\"add\"])\n\tadd_list := object.get(container, path_to_add, [])\n\t\"NET_RAW\" in add_list\n\tfailed_path := [sprintf(\"%s[%d].%s\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_add)])]\n\tfix_path := \"\"\n}\n\nall_in_list(list) {\n\t\"all\" in list\n}\n\nall_in_list(list) {\n\t\"ALL\" in list\n}\n\n\nget_failed_path(paths) = paths[0] {\n\tpaths[0] != \"\"\n} else = []\n\n\nget_fixed_path(paths) = paths[1] {\n\tpaths[1] != \"\"\n} else = []\n\n"
                },
                {
                    "name": "set-seLinuxOptions",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if workload and container do not define any seLinuxOptions",
                    "remediation": "Make sure you set seLinuxOptions in the workload/container security context.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# Fails if pod does not define seLinuxOptions \ndeny[msga] {\n    wl := input[_]\n    wl.kind == \"Pod\"\n    spec := wl.spec\n\tpath_to_search := [\"securityContext\", \"seLinuxOptions\"]\n\tno_seLinuxOptions_in_securityContext(spec, path_to_search)\n\n\tpath_to_containers := [\"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n    no_seLinuxOptions_in_securityContext(container, path_to_search)\n\n\tfix_path := sprintf(\"%s[%d].%s\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_search)]) \n\tfixPaths := [{\"path\": fix_path, \"value\": \"YOUR_VALUE\"}]\n\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v does not define any seLinuxOptions\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if workload does not define seLinuxOptions\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    spec := wl.spec.template.spec\n\tpath_to_search := [\"securityContext\", \"seLinuxOptions\"]\n\tno_seLinuxOptions_in_securityContext(spec, path_to_search)\n\n\tpath_to_containers := [\"spec\", \"template\", \"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n    no_seLinuxOptions_in_securityContext(container, path_to_search)\n\n\tfix_path := sprintf(\"%s[%d].%s\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_search)]) \n\tfixPaths := [{\"path\": fix_path, \"value\": \"YOUR_VALUE\"}]\n\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v does not define any seLinuxOptions\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if CronJob does not define seLinuxOptions \ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tspec := wl.spec.jobTemplate.spec.template.spec\n\tpath_to_search := [\"securityContext\", \"seLinuxOptions\"]\n\tno_seLinuxOptions_in_securityContext(spec, path_to_search)\n\n\tpath_to_containers := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n    no_seLinuxOptions_in_securityContext(container, path_to_search)\n\n\tfix_path := sprintf(\"%s[%d].%s\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_search)]) \n\tfixPaths := [{\"path\": fix_path, \"value\": \"YOUR_VALUE\"}]\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Cronjob: %v does not define any seLinuxOptions\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\nno_seLinuxOptions_in_securityContext(spec, path_to_search){\n    object.get(spec, path_to_search, \"\") == \"\"\n}"
                },
                {
                    "name": "set-seccomp-profile",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container does not define seccompProfile",
                    "remediation": "Make sure you define seccompProfile at workload or container lever.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# Fails if pod does not define seccompProfile\ndeny[msga] {\n    wl := input[_]\n    wl.kind == \"Pod\"\n    spec := wl.spec\n\tpath_to_search := [\"securityContext\", \"seccompProfile\"]\n\tseccompProfile_not_defined(spec, path_to_search)\n\n\tpath_to_containers := [\"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n    seccompProfile_not_defined(container, path_to_search)\n\n\tfix_path := sprintf(\"%s[%d].%s\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_search)]) \n\tfixPaths := [{\"path\": fix_path, \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v does not define seccompProfile\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if workload does not define seccompProfile\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    spec := wl.spec.template.spec\n\tpath_to_search := [\"securityContext\", \"seccompProfile\"]\n\tseccompProfile_not_defined(spec, path_to_search)\n\n\tpath_to_containers := [\"spec\", \"template\", \"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n    seccompProfile_not_defined(container, path_to_search)\n\n\tfix_path := sprintf(\"%s[%d].%s\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_search)]) \n\tfixPaths := [{\"path\": fix_path, \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v does not define seccompProfile\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if CronJob does not define seccompProfile\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n    spec := wl.spec.jobTemplate.spec.template.spec\n\tpath_to_search := [\"securityContext\", \"seccompProfile\"]\n\tseccompProfile_not_defined(spec, path_to_search)\n\n\tpath_to_containers := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"containers\"]\n\tcontainers := object.get(wl, path_to_containers, [])\n\tcontainer := containers[i]\n    seccompProfile_not_defined(container, path_to_search)\n\n\tfix_path := sprintf(\"%s[%d].%s\", [concat(\".\", path_to_containers), i, concat(\".\", path_to_search)]) \n\tfixPaths := [{\"path\": fix_path, \"value\": \"YOUR_VALUE\"}]\n\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Cronjob: %v does not define seccompProfile\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\nseccompProfile_not_defined(spec, path_to_search){\n\tobject.get(spec, path_to_search, \"\") == \"\"\n}"
                },
                {
                    "name": "set-procmount-default",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                "hostdata.kubescape.cloud"
                            ],
                            "apiVersions": [
                                "v1beta0"
                            ],
                            "resources": [
                                "ControlPlaneInfo"
                            ]
                        },
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Fails if container does not define securityContext.procMount to Default.",
                    "remediation": "Set securityContext.procMount to Default",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.if\n\n# Fails if container does not define the \"procMount\" parameter as \"Default\"\ndeny[msga] {\n\t# checks at first if we the procMountType feature gate is enabled on the api-server\n\tobj := input[_]\n\tis_control_plane_info(obj)\n\tis_proc_mount_type_enabled(obj.data.APIServerInfo.cmdLine)\n\n\t# checks if procMount paramenter has the right value in containers\n\tpod := input[_]\n\tpod.kind = \"Pod\"\n\n\t# retrieve container list\n\tcontainer := pod.spec.containers[i]\n\tnot procMountSetProperly(container.securityContext)\n\n\tfixPaths = [{\"path\": sprintf(\"spec.containers[%d].securityContext.procMount\", [i]), \"value\": \"Default\"}]\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v has containers that do not set 'securityContext.procMount' to 'Default'\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [pod]},\n\t}\n}\n\ndeny[msga] {\n\t# checks at first if we the procMountType feature gate is enabled on the api-server\n\tobj := input[_]\n\tis_control_plane_info(obj)\n\tis_proc_mount_type_enabled(obj.data.APIServerInfo.cmdLine)\n\n\t# checks if we are managing the right workload kind\n\twl := input[_]\n\tmanifest_kind := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Job\"}\n\tmanifest_kind[wl.kind]\n\n\t# retrieve container list\n\tcontainer := wl.spec.template.spec.containers[i]\n\tnot procMountSetProperly(container.securityContext)\n\n\tfixPaths = [{\"path\": sprintf(\"spec.template.spec.containers[%d].securityContext.procMount\", [i]), \"value\": \"Default\"}]\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v has containers that do not set 'securityContext.procMount' to 'Default'\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\ndeny[msga] {\n\t# checks at first if we the procMountType feature gate is enabled on the api-server\n\tobj := input[_]\n\tis_control_plane_info(obj)\n\tis_proc_mount_type_enabled(obj.data.APIServerInfo.cmdLine)\n\n\t# checks if we are managing the right workload kind\n\tcj := input[_]\n\tcj.kind = \"CronJob\"\n\n\t# retrieve container list\n\tcontainer := cj.spec.jobTemplate.spec.template.spec.containers[i]\n\tnot procMountSetProperly(container.securityContext)\n\n\tfixPaths = [{\"path\": sprintf(\"spec.jobTemplate.spec.template.spec.containers[%d].securityContext.procMount\", [i]), \"value\": \"Default\"}]\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"CronJob: %v has containers that do not set 'securityContext.procMount' to 'Default'\", [cj.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [cj]},\n\t}\n}\n\n# check if we are managing ControlPlaneInfo\nis_control_plane_info(obj) if {\n\tobj.apiVersion == \"hostdata.kubescape.cloud/v1beta0\"\n\tobj.kind == \"ControlPlaneInfo\"\n}\n\n# check if ProcMountType feature-gate is enabled\nis_proc_mount_type_enabled(command) if {\n\tcontains(command, \"--feature-gates=\")\n\targs := regex.split(` +`, command)\n\tsome i\n\tregex.match(`ProcMountType=true`, args[i])\n}\n\n# procMountSetProperly checks if procMount has value of \"Default\".\nprocMountSetProperly(securityContext) if {\n\tsecurityContext.procMount == \"Default\"\n} else := false\n"
                },
                {
                    "name": "set-fsgroup-value",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Fails if securityContext.fsGroup is not set.",
                    "remediation": "Set securityContext.fsGroup value",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.if\n\n### POD ###\n\n# Fails if securityContext.fsGroup does not have a values >= 0\ndeny[msga] {\n\t# verify the object kind\n\tpod := input[_]\n\tpod.kind = \"Pod\"\n\n\t# check securityContext has fsGroup set properly\n\tnot fsGroupSetProperly(pod.spec.securityContext)\n\n\tsecurityContextPath := \"spec.securityContext\"\n\n\tfixPaths = [{\"path\": sprintf(\"%v.fsGroup\", [securityContextPath]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v does not set 'securityContext.fsGroup' with allowed value\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [pod]},\n\t}\n}\n\n### CRONJOB ###\n\n# Fails if securityContext.fsGroup does not have a values >= 0\ndeny[msga] {\n\t# verify the object kind\n\tcj := input[_]\n\tcj.kind == \"CronJob\"\n\n\t# check securityContext has fsGroup set properly\n\tnot fsGroupSetProperly(cj.spec.jobTemplate.spec.template.spec.securityContext)\n\n\tsecurityContextPath := \"spec.jobTemplate.spec.template.spec.securityContext\"\n\n\tfixPaths = [{\"path\": sprintf(\"%v.fsGroup\", [securityContextPath]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"CronJob: %v does not set 'securityContext.fsGroup' with allowed value\", [cj.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [cj]},\n\t}\n}\n\n### WORKLOAD ###\n\n# Fails if securityContext.fsGroup does not have a values >= 0\ndeny[msga] {\n\t# verify the object kind\n\twl := input[_]\n\tmanifest_kind := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Job\"}\n\tmanifest_kind[wl.kind]\n\n\t# check securityContext has fsGroup set properly\n\tnot fsGroupSetProperly(wl.spec.template.spec.securityContext)\n\n\tsecurityContextPath := \"spec.template.spec.securityContext\"\n\tfixPaths = [{\"path\": sprintf(\"%v.fsGroup\", [securityContextPath]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v does not set 'securityContext.fsGroup' with allowed value\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\n# fsGroupSetProperly checks if fsGroup has a value >= 0.\nfsGroupSetProperly(securityContext) if {\n\tsecurityContext.fsGroup >= 0\n} else := false\n"
                },
                {
                    "name": "set-fsgroupchangepolicy-value",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Fails if securityContext.fsGroup is not set.",
                    "remediation": "Set securityContext.fsGroup value",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.if\n\n### POD ###\n\n# Fails if securityContext.fsGroupChangePolicy does not have an allowed value\ndeny[msga] {\n    # verify the object kind\n    pod := input[_]\n    pod.kind = \"Pod\"\n    \n    # check securityContext has fsGroupChangePolicy set\n    not fsGroupChangePolicySetProperly(pod.spec.securityContext)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v does not set 'securityContext.fsGroupChangePolicy' with allowed value\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [{\"path\": \"spec.securityContext.fsGroupChangePolicy\", \"value\": \"Always\"}],\n    \"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n    }\n}\n\n### WORKLOAD ###\n\n# Fails if securityContext.fsGroupChangePolicy does not have an allowed value\ndeny[msga] {\n    # verify the object kind\n    wl := input[_]\n    manifest_kind := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n    manifest_kind[wl.kind]\n    \n    # check securityContext has fsGroupChangePolicy set\n    not fsGroupChangePolicySetProperly(wl.spec.template.spec.securityContext)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v does not set 'securityContext.fsGroupChangePolicy' with allowed value\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [{\"path\": \"spec.template.spec.securityContext.fsGroupChangePolicy\", \"value\": \"Always\"}],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n    }\n}\n\n### CRONJOB ###\n\n# Fails if securityContext.fsGroupChangePolicy does not have an allowed value\ndeny[msga] {\n    # verify the object kind\n    cj := input[_]\n    cj.kind == \"CronJob\"\n\n    # check securityContext has fsGroupChangePolicy set\n    not fsGroupChangePolicySetProperly(cj.spec.jobTemplate.spec.template.spec.securityContext)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"CronJob: %v does not set 'securityContext.fsGroupChangePolicy' with allowed value\", [cj.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [{\"path\": \"spec.jobTemplate.spec.template.spec.securityContext.fsGroupChangePolicy\", \"value\": \"Always\"}],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [cj]\n\t\t}\n    }\n}\n\n# fsGroupChangePolicySetProperly checks if applied value is set as appropriate [Always|OnRootMismatch]\nfsGroupChangePolicySetProperly(securityContext) := true if {\n    regex.match(securityContext.fsGroupChangePolicy, \"Always|OnRootMismatch\")\n} else := false\n\n"
                },
                {
                    "name": "set-sysctls-params",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Fails if securityContext.sysctls is not set.",
                    "remediation": "Set securityContext.sysctls params",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n### POD ###\n\n# Fails if securityContext.sysctls is not set\ndeny[msga] {\n    # verify the object kind\n\tpod := input[_]\n\tpod.kind = \"Pod\"\n\n\t# check securityContext has sysctls set\n    not pod.spec.securityContext.sysctls\n\n    path := \"spec.securityContext.sysctls\"\n\tfixPaths := [{\"path\": sprintf(\"%s.name\", [path]), \"value\": \"YOUR_VALUE\"},\n\t\t\t\t{\"path\": sprintf(\"%s.value\", [path]), \"value\": \"YOUR_VALUE\"}]\n    msga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v does not set 'securityContext.sysctls'\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n    }\n}\n\n### WORKLOAD ###\n\n# Fails if securityContext.sysctls is not set\ndeny[msga] {\n    # verify the object kind\n\twl := input[_]\n\tmanifest_kind := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tmanifest_kind[wl.kind]\n\n\t# check securityContext has sysctls set\n    not wl.spec.template.spec.securityContext.sysctls\n\n    path := \"spec.template.spec.securityContext.sysctls\"\n\tfixPaths := [{\"path\": sprintf(\"%s.name\", [path]), \"value\": \"YOUR_VALUE\"},\n\t\t\t\t{\"path\": sprintf(\"%s.value\", [path]), \"value\": \"YOUR_VALUE\"}]\n    msga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v does not set 'securityContext.sysctls'\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n    }\n}\n\n### CRONJOB ###\n\n# Fails if securityContext.sysctls is not set\ndeny[msga] {\n    # verify the object kind\n\tcj := input[_]\n    cj.kind == \"CronJob\"\n\n\t# check securityContext has sysctls set\n    not cj.spec.jobTemplate.spec.template.spec.securityContext.sysctls\n\n    path := \"spec.jobTemplate.spec.template.spec.securityContext.sysctls\"\n\tfixPaths := [{\"path\": sprintf(\"%s.name\", [path]), \"value\": \"YOUR_VALUE\"},\n\t\t\t\t{\"path\": sprintf(\"%s.value\", [path]), \"value\": \"YOUR_VALUE\"}]\n    msga := {\n\t\t\"alertMessage\": sprintf(\"CronJob: %v does not set 'securityContext.sysctls'\", [cj.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [cj]\n\t\t}\n    }\n}\n"
                },
                {
                    "name": "set-supplementalgroups-values",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Fails if securityContext.supplementalgroups is not set.",
                    "remediation": "Set securityContext.supplementalgroups values",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n### POD ###\n\n# Fails if securityContext.supplementalGroups is not set\ndeny[msga] {\n\t# verify the object kind\n\tpod := input[_]\n\tpod.kind = \"Pod\"\n\n\t# check securityContext has supplementalGroups set\n\tnot pod.spec.securityContext.supplementalGroups\n\tfixPaths = [{\"path\": \"spec.securityContext.supplementalGroups\", \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Pod: %v does not set 'securityContext.supplementalGroups'\", [pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [pod]},\n\t}\n}\n\n### WORKLOAD ###\n\n# Fails if securityContext.supplementalGroups is not set\ndeny[msga] {\n\t# verify the object kind\n\twl := input[_]\n\tmanifest_kind := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Job\"}\n\tmanifest_kind[wl.kind]\n\n\t# check securityContext has supplementalGroups set\n\tnot wl.spec.template.spec.securityContext.supplementalGroups\n\tfixPaths = [{\"path\": \"spec.template.spec.securityContext.supplementalGroups\", \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Workload: %v does not set 'securityContext.supplementalGroups'\", [wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\n### CRONJOB ###\n\n# Fails if securityContext.supplementalGroups is not set\ndeny[msga] {\n\t# verify the object kind\n\tcj := input[_]\n\tcj.kind == \"CronJob\"\n\n\t# check securityContext has supplementalGroups set\n\tnot cj.spec.jobTemplate.spec.template.spec.securityContext.supplementalGroups\n\tfixPaths = [{\"path\": \"spec.jobTemplate.spec.template.spec.securityContext.supplementalGroups\", \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"CronJob: %v does not set 'securityContext.supplementalGroups'\", [cj.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\"k8sApiObjects\": [cj]},\n\t}\n}\n"
                },
                {
                    "name": "rule-allow-privilege-escalation",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "policy"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "PodSecurityPolicy"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if container allows privilege escalation",
                    "remediation": "Make sure that the allowPrivilegeEscalation field in the securityContext of pod/container is set to false",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# Fails if pod has container  that allow privilege escalation\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tstart_of_path := \"spec.\"\n    is_allow_privilege_escalation_container(container)\n\tfixPath := get_fix_path(i, start_of_path)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"container: %v in pod: %v  allow privilege escalation\", [container.name, pod.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPath,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n\n# Fails if workload has a container that allow privilege escalation\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.template.spec.\"\n    is_allow_privilege_escalation_container(container)\n\tfixPath := get_fix_path(i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v  allow privilege escalation\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPath,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n# Fails if cronjob has a container that allow privilege escalation\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tstart_of_path := \"spec.jobTemplate.spec.template.spec.\"\n\tis_allow_privilege_escalation_container(container)\n\tfixPath := get_fix_path(i, start_of_path)\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"container :%v in %v: %v allow privilege escalation\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPath,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n\nis_allow_privilege_escalation_container(container) {\n    not container.securityContext.allowPrivilegeEscalation == false\n\tnot container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) == 0\n}\n\nis_allow_privilege_escalation_container(container) {\n    not container.securityContext.allowPrivilegeEscalation == false\n\tnot container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) > 0\n\tpsp := psps[_]\n\tnot psp.spec.allowPrivilegeEscalation == false\n}\n\n\nis_allow_privilege_escalation_container(container) {\n    container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) == 0\n}\n\nis_allow_privilege_escalation_container(container) {\n    container.securityContext.allowPrivilegeEscalation == true\n\tpsps := [psp |  psp= input[_]; psp.kind == \"PodSecurityPolicy\"]\n\tcount(psps) > 0\n\tpsp := psps[_]\n\tnot psp.spec.allowPrivilegeEscalation == false\n}\n\nget_fix_path(i, start_of_path) = [{\"path\": sprintf(\"%vcontainers[%v].securityContext.allowPrivilegeEscalation\", [start_of_path, i]), \"value\":\"false\"},\n\t{\"path\": sprintf(\"%vcontainers[%v].securityContext.privileged\", [start_of_path, i]), \"value\":\"false\"}]\n"
                }
            ]
        },
        {
            "name": "Workload with secret access",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Secret Access"
                        ]
                    }
                ]
            },
            "description": "This control identifies workloads that have mounted secrets. Workloads with secret access can potentially expose sensitive information and increase the risk of unauthorized access to critical resources.",
            "remediation": "Review the workloads identified by this control and assess whether it's necessary to mount these secrets. Remove secret access from workloads that don't require it or ensure appropriate access controls are in place to protect sensitive information.",
            "test": "Check if any workload has mounted secrets by inspecting their specifications and verifying if secret volumes are defined.",
            "controlID": "C-0255",
            "baseScore": 8.0,
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "workload-mounted-secrets",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "Secret"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "description": "fails if workload mounts secrets",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\ndeny[msga] {\n\tresource := input[_]\n\tvolumes_path := get_volumes_path(resource)\n\tvolumes := object.get(resource, volumes_path, [])\n\tvolume := volumes[i]\n\tvolume.secret\n\n\tsecret := input[_]\n\tsecret.kind == \"Secret\"\n\tsecret.metadata.name == volume.secret.secretName\n\tis_same_namespace(secret.metadata, resource.metadata)\n\n\tcontainers_path := get_containers_path(resource)\n\tcontainers := object.get(resource, containers_path, [])\n\tcontainer := containers[j]\n\tcontainer.volumeMounts\n\n \t# check if volume is mounted\n\tcontainer.volumeMounts[k].name == volume.name\n\n\tfailedPaths := sprintf(\"%s[%d].volumeMounts[%d]\", [concat(\".\", containers_path), j, k])\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v has mounted secret\", [resource.kind, resource.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"deletePaths\": [failedPaths],\n\t\t\"failedPaths\": [failedPaths],\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [resource]\n\t\t},\n        \"relatedObjects\": [{\n            \"object\": secret\n        }]\n\t}\n}\n\n# get_volume_path - get resource volumes paths for {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\nget_volumes_path(resource) := result {\n\tresource_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tresource_kinds[resource.kind]\n\tresult = [\"spec\", \"template\", \"spec\", \"volumes\"]\n}\n\n# get_containers_path - get resource containers paths for  {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\nget_containers_path(resource) := result {\n\tresource_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tresource_kinds[resource.kind]\n\tresult = [\"spec\", \"template\", \"spec\", \"containers\"]\n}\n\n# get_containers_path - get resource containers paths for \"Pod\"\nget_containers_path(resource) := result {\n\tresource.kind == \"Pod\"\n\tresult = [\"spec\", \"containers\"]\n}\n\n# get_containers_path - get resource containers paths for  \"CronJob\"\nget_containers_path(resource) := result {\n\tresource.kind == \"CronJob\"\n\tresult = [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"containers\"]\n}\n\n# get_volumes_path - get resource volumes paths for \"Pod\"\nget_volumes_path(resource) := result {\n\tresource.kind == \"Pod\"\n\tresult = [\"spec\", \"volumes\"]\n}\n\n# get_volumes_path - get resource volumes paths for \"CronJob\"\nget_volumes_path(resource) := result {\n\tresource.kind == \"CronJob\"\n\tresult = [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"volumes\"]\n}\n\n\n\nis_same_namespace(metadata1, metadata2) {\n\tmetadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tnot metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata2.namespace\n\tmetadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tmetadata2.namespace == \"default\"\n}"
                }
            ]
        },
        {
            "name": "External facing",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Initial Access"
                        ]
                    },
                    {
                        "attackTrack": "service-destruction",
                        "categories": [
                            "Initial Access"
                        ]
                    },
                    {
                        "attackTrack": "external-workload-with-cluster-takeover-roles",
                        "categories": [
                            "Initial Access"
                        ]
                    },
                    {
                        "attackTrack": "external-database-without-authentication",
                        "categories": [
                            "Initial Access"
                        ]
                    },
                    {
                        "attackTrack": "workload-unauthenticated-service",
                        "categories": [
                            "Initial Access"
                        ]
                    }
                ]
            },
            "description": "This control detect workloads that are exposed on Internet through a Service (NodePort or LoadBalancer) or Ingress. It fails in case it find workloads connected with these resources.",
            "remediation": "The user can evaluate its exposed resources and apply relevant changes wherever needed.",
            "test": "Checks if workloads are exposed through the use of NodePort, LoadBalancer or Ingress",
            "controlID": "C-0256",
            "baseScore": 7.0,
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "exposure-to-internet",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "Service"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "networking.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Ingress"
                            ]
                        }
                    ],
                    "description": "fails in case the running workload has binded Service or Ingress that are exposing it on Internet.",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# Checks if NodePort or LoadBalancer is connected to a workload to expose something\ndeny[msga] {\n    service := input[_]\n    service.kind == \"Service\"\n    is_exposed_service(service)\n\n    wl := input[_]\n    spec_template_spec_patterns := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Pod\", \"Job\", \"CronJob\"}\n    spec_template_spec_patterns[wl.kind]\n    wl_connected_to_service(wl, service)\n    failPath := [\"spec.type\"]\n    msga := {\n        \"alertMessage\": sprintf(\"workload '%v' is exposed through service '%v'\", [wl.metadata.name, service.metadata.name]),\n        \"packagename\": \"armo_builtins\",\n        \"alertScore\": 7,\n        \"fixPaths\": [],\n        \"failedPaths\": [],\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n        \"relatedObjects\": [{\n            \"object\": service,\n\t\t    \"reviewPaths\": failPath,\n            \"failedPaths\": failPath,\n        }]\n    }\n}\n\n# Checks if Ingress is connected to a service and a workload to expose something\ndeny[msga] {\n    ingress := input[_]\n    ingress.kind == \"Ingress\"\n\n    svc := input[_]\n    svc.kind == \"Service\"\n\n    # Make sure that they belong to the same namespace\n    svc.metadata.namespace == ingress.metadata.namespace\n\n    # avoid duplicate alerts\n    # if service is already exposed through NodePort or LoadBalancer workload will fail on that\n    not is_exposed_service(svc)\n\n    wl := input[_]\n    spec_template_spec_patterns := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Pod\", \"Job\", \"CronJob\"}\n    spec_template_spec_patterns[wl.kind]\n    wl_connected_to_service(wl, svc)\n\n    result := svc_connected_to_ingress(svc, ingress)\n\n    msga := {\n        \"alertMessage\": sprintf(\"workload '%v' is exposed through ingress '%v'\", [wl.metadata.name, ingress.metadata.name]),\n        \"packagename\": \"armo_builtins\",\n        \"failedPaths\": [],\n        \"fixPaths\": [],\n        \"alertScore\": 7,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n        \"relatedObjects\": [\n\t\t{\n\t            \"object\": ingress,\n\t\t    \"reviewPaths\": result,\n\t            \"failedPaths\": result,\n\t        },\n\t\t{\n\t            \"object\": svc,\n\t\t}\n        ]\n    }\n}\n\n# ====================================================================================\n\nis_exposed_service(svc) {\n    svc.spec.type == \"NodePort\"\n}\n\nis_exposed_service(svc) {\n    svc.spec.type == \"LoadBalancer\"\n}\n\nwl_connected_to_service(wl, svc) {\n    count({x | svc.spec.selector[x] == wl.metadata.labels[x]}) == count(svc.spec.selector)\n}\n\nwl_connected_to_service(wl, svc) {\n    wl.spec.selector.matchLabels == svc.spec.selector\n}\n\nwl_connected_to_service(wl, svc) {\n    count({x | svc.spec.selector[x] == wl.spec.template.metadata.labels[x]}) == count(svc.spec.selector)\n}\n\n# check if service is connected to ingress\nsvc_connected_to_ingress(svc, ingress) = result {\n    rule := ingress.spec.rules[i]\n    paths := rule.http.paths[j]\n    svc.metadata.name == paths.backend.service.name\n    result := [sprintf(\"spec.rules[%d].http.paths[%d].backend.service.name\", [i,j])]\n}\n\n\n"
                }
            ]
        },
        {
            "name": "Workload with PVC access",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Data Collection"
                        ]
                    }
                ]
            },
            "description": "This control detects workloads that have mounted PVC. Workloads with PVC access can potentially expose sensitive information and elevate the risk of unauthorized access to critical resources.",
            "remediation": "Review the workloads identified by this control and assess whether it's necessary to mount these PVCs. Remove PVC access from workloads that don't require it or ensure appropriate access controls are in place to protect sensitive information.",
            "test": "Check if any workload has mounted PVCs by inspecting their specifications and verifying if PVC volumes are defined",
            "controlID": "C-0257",
            "baseScore": 4.0,
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Storage",
                    "id": "Cat-8"
                },
                "id": "Cat-5"
            },
            "rules": [
                {
                    "name": "workload-mounted-pvc",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "ConfigMap"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "description": "fails if workload mounts PVC",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\ndeny[msga] {\n\tresource := input[_]\n\tvolumes_path := get_volumes_path(resource)\n\tvolumes := object.get(resource, volumes_path, [])\n\tvolume := volumes[i]\n\tvolume.persistentVolumeClaim\n\n\tPVC := input[_]\n\tPVC.kind == \"PersistentVolumeClaim\"\n\tPVC.metadata.name == volume.persistentVolumeClaim.claimName\n\tis_same_namespace(PVC.metadata, resource.metadata)\n\n\tcontainers_path := get_containers_path(resource)\n\tcontainers := object.get(resource, containers_path, [])\n\tcontainer := containers[j]\n\tcontainer.volumeMounts\n\n \t# check if volume is mounted\n\tcontainer.volumeMounts[k].name == volume.name\n\n\tfailedPaths := sprintf(\"%s[%d].volumeMounts[%d]\", [concat(\".\", containers_path), j, k])\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v has mounted PVC\", [resource.kind, resource.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"deletePaths\": [failedPaths],\n\t\t\"failedPaths\": [failedPaths],\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [resource]\n\t\t},\n        \"relatedObjects\": [{\n            \"object\": PVC\n        }]\n\t}\n}\n\n\n# get_containers_path - get resource containers paths for  {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\nget_containers_path(resource) := result {\n\tresource_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tresource_kinds[resource.kind]\n\tresult = [\"spec\", \"template\", \"spec\", \"containers\"]\n}\n\n# get_containers_path - get resource containers paths for \"Pod\"\nget_containers_path(resource) := result {\n\tresource.kind == \"Pod\"\n\tresult = [\"spec\", \"containers\"]\n}\n\n# get_containers_path - get resource containers paths for  \"CronJob\"\nget_containers_path(resource) := result {\n\tresource.kind == \"CronJob\"\n\tresult = [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"containers\"]\n}\n\n# get_volume_path - get resource volumes paths for {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\nget_volumes_path(resource) := result {\n\tresource_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tresource_kinds[resource.kind]\n\tresult = [\"spec\", \"template\", \"spec\", \"volumes\"]\n}\n\n# get_volumes_path - get resource volumes paths for \"Pod\"\nget_volumes_path(resource) := result {\n\tresource.kind == \"Pod\"\n\tresult = [\"spec\", \"volumes\"]\n}\n\n# get_volumes_path - get resource volumes paths for \"CronJob\"\nget_volumes_path(resource) := result {\n\tresource.kind == \"CronJob\"\n\tresult = [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"volumes\"]\n}\n\n\n\nis_same_namespace(metadata1, metadata2) {\n\tmetadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tnot metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata2.namespace\n\tmetadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tmetadata2.namespace == \"default\"\n}"
                }
            ]
        },
        {
            "name": "Workload with configMap access",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Data Collection"
                        ]
                    }
                ]
            },
            "description": "This control detects workloads that have mounted ConfigMaps. Workloads with ConfigMap access can potentially expose sensitive information and elevate the risk of unauthorized access to critical resources.",
            "remediation": "Review the workloads identified by this control and assess whether it's necessary to mount these configMaps. Remove configMaps access from workloads that don't require it or ensure appropriate access controls are in place to protect sensitive information.",
            "test": "Check if any workload has mounted secrets by inspecting their specifications and verifying if secret volumes are defined",
            "controlID": "C-0258",
            "baseScore": 5.0,
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "workload-mounted-configmap",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "ConfigMap"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "description": "fails if workload mounts ConfigMaps",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\ndeny[msga] {\n\tresource := input[_]\n\tvolumes_path := get_volumes_path(resource)\n\tvolumes := object.get(resource, volumes_path, [])\n\tvolume := volumes[i]\n\tvolume.configMap\n\n\tconfigMap := input[_]\n\tconfigMap.kind == \"ConfigMap\"\n\tconfigMap.metadata.name == volume.configMap.name\n\tis_same_namespace(configMap.metadata, resource.metadata)\n\n\tcontainers_path := get_containers_path(resource)\n\tcontainers := object.get(resource, containers_path, [])\n\tcontainer := containers[j]\n\tcontainer.volumeMounts\n\n \t# check if volume is mounted\n\tcontainer.volumeMounts[k].name == volume.name\n\n\tfailedPaths := sprintf(\"%s[%d].volumeMounts[%d]\", [concat(\".\", containers_path), j, k])\n\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"%v: %v has mounted configMap\", [resource.kind, resource.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"deletePaths\": [failedPaths],\n\t\t\"failedPaths\": [failedPaths],\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [resource]\n\t\t},\n        \"relatedObjects\": [{\n            \"object\": configMap\n        }]\n\t}\n}\n\n\n\n# get_containers_path - get resource containers paths for  {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\nget_containers_path(resource) := result {\n\tresource_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tresource_kinds[resource.kind]\n\tresult = [\"spec\", \"template\", \"spec\", \"containers\"]\n}\n\n# get_containers_path - get resource containers paths for \"Pod\"\nget_containers_path(resource) := result {\n\tresource.kind == \"Pod\"\n\tresult = [\"spec\", \"containers\"]\n}\n\n# get_containers_path - get resource containers paths for  \"CronJob\"\nget_containers_path(resource) := result {\n\tresource.kind == \"CronJob\"\n\tresult = [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"containers\"]\n}\n\n# get_volume_path - get resource volumes paths for {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\nget_volumes_path(resource) := result {\n\tresource_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tresource_kinds[resource.kind]\n\tresult = [\"spec\", \"template\", \"spec\", \"volumes\"]\n}\n\n# get_volumes_path - get resource volumes paths for \"Pod\"\nget_volumes_path(resource) := result {\n\tresource.kind == \"Pod\"\n\tresult = [\"spec\", \"volumes\"]\n}\n\n# get_volumes_path - get resource volumes paths for \"CronJob\"\nget_volumes_path(resource) := result {\n\tresource.kind == \"CronJob\"\n\tresult = [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\", \"volumes\"]\n}\n\n\n\nis_same_namespace(metadata1, metadata2) {\n\tmetadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tnot metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata2.namespace\n\tmetadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tmetadata2.namespace == \"default\"\n}"
                }
            ]
        },
        {
            "name": "Workload with credential access",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Credential access"
                        ]
                    }
                ]
            },
            "description": "This control checks if workloads specifications have sensitive information in their environment variables.",
            "remediation": "Use Kubernetes secrets or Key Management Systems to store credentials.",
            "test": "Check if the workload has sensitive information in environment variables, by using list of known sensitive key names.",
            "controlID": "C-0259",
            "baseScore": 8.0,
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "rule-credentials-in-env-var",
                    "attributes": {
                        "m$K8sThreatMatrix": "Credential access::Applications credentials in configuration files, Lateral Movement::Applications credentials in configuration files"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "configInputs": [
                        "settings.postureControlInputs.sensitiveValues",
                        "settings.postureControlInputs.sensitiveKeyNames",
                        "settings.postureControlInputs.sensitiveValuesAllowed",
                        "settings.postureControlInputs.sensitiveKeyNamesAllowed"
                    ],
                    "controlConfigInputs": [
                        {
                            "path": "settings.postureControlInputs.sensitiveValues",
                            "name": "Sensitive Values",
                            "description": "Strings that identify a value that Kubescape believes should be stored in a Secret, and not in a ConfigMap or an environment variable."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveValuesAllowed",
                            "name": "Allowed Values",
                            "description": "Reduce false positives with known values."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveKeyNames",
                            "name": "Sensitive Keys",
                            "description": "Key names that identify a potential value that should be stored in a Secret, and not in a ConfigMap or an environment variable."
                        },
                        {
                            "path": "settings.postureControlInputs.sensitiveKeyNamesAllowed",
                            "name": "Allowed Keys",
                            "description": "Reduce false positives with known key names."
                        }
                    ],
                    "description": "fails if Pods have sensitive information in configuration",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "\tpackage armo_builtins\n\n\tdeny[msga] {\n\t\tpod := input[_]\n\t\tpod.kind == \"Pod\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_key_names := data.postureControlInputs.sensitiveKeyNames\n\t\tkey_name := sensitive_key_names[_]\n\t\tcontainer := pod.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.name), lower(key_name))\n\t\tenv.value != \"\"\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Pod: %v has sensitive information in environment variables\", [pod.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [pod]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\t\tspec_template_spec_patterns[wl.kind]\n\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_key_names := data.postureControlInputs.sensitiveKeyNames\n\t\tkey_name := sensitive_key_names[_]\n\t\tcontainer := wl.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.name), lower(key_name))\n\t\tenv.value != \"\"\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\tsprintf(\"spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"%v: %v has sensitive information in environment variables\", [wl.kind, wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\twl.kind == \"CronJob\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_key_names := data.postureControlInputs.sensitiveKeyNames\n\t\tkey_name := sensitive_key_names[_]\n\t\tcontainer := wl.spec.jobTemplate.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.name), lower(key_name))\n\t\tenv.value != \"\"\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Cronjob: %v has sensitive information in environment variables\", [wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n# check sensitive values\ndeny[msga] {\n\t\tpod := input[_]\n\t\tpod.kind == \"Pod\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_values := data.postureControlInputs.sensitiveValues\n    \tvalue := sensitive_values[_]\n\t\tcontainer := pod.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.value), lower(value))\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Pod: %v has sensitive information in environment variables\", [pod.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [pod]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\t\tspec_template_spec_patterns[wl.kind]\n\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_values := data.postureControlInputs.sensitiveValues\n    \tvalue := sensitive_values[_]\n\t\tcontainer := wl.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.value), lower(value))\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\tsprintf(\"spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"%v: %v has sensitive information in environment variables\", [wl.kind, wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n\tdeny[msga] {\n\t\twl := input[_]\n\t\twl.kind == \"CronJob\"\n\t\t# see default-config-inputs.json for list values\n\t\tsensitive_values := data.postureControlInputs.sensitiveValues\n    \tvalue := sensitive_values[_]\n\t\tcontainer := wl.spec.jobTemplate.spec.template.spec.containers[i]\n\t\tenv := container.env[j]\n\n\t\tcontains(lower(env.value), lower(value))\n\t\t# check that value or key weren't allowed by user\n    \tnot is_allowed_value(env.value)\n    \tnot is_allowed_key_name(env.name)\n\n\t\tis_not_reference(env)\n\n\t\tpaths := [sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].name\", [i, j]),\n\t\t\t\t  sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].env[%v].value\", [i, j])]\n\n\t\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Cronjob: %v has sensitive information in environment variables\", [wl.metadata.name]),\n\t\t\t\"alertScore\": 9,\n\t\t\t\"fixPaths\": [],\n\t\t\t\"deletePaths\": paths,\n\t\t\t\"failedPaths\": paths,\n\t\t\t\"packagename\": \"armo_builtins\",\n\t\t\t\"alertObject\": {\n\t\t\t\t\"k8sApiObjects\": [wl]\n\t\t\t}\n\t\t}\n\t}\n\n\nis_not_reference(env)\n{\n\tnot env.valueFrom.secretKeyRef\n\tnot env.valueFrom.configMapKeyRef\n}\n\nis_allowed_value(value) {\n    allow_val := data.postureControlInputs.sensitiveValuesAllowed[_]\n    regex.match(allow_val , value)\n}\n\nis_allowed_key_name(key_name) {\n    allow_key := data.postureControlInputs.sensitiveKeyNamesAllowed[_]\n    contains(lower(key_name), lower(allow_key))\n}"
                }
            ]
        },
        {
            "name": "Missing network policy",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Lateral Movement (Network)"
                        ]
                    }
                ],
                "isFixedByNetworkPolicy": true
            },
            "description": "This control detects workloads that has no NetworkPolicy configured in labels. If a network policy is not configured, it means that your applications might not have necessary control over the traffic to and from the pods, possibly leading to a security vulnerability.",
            "remediation": "Review the workloads identified by this control and assess whether it's necessary to configure a network policy for them.",
            "test": "Check that all workloads has a network policy configured in labels.",
            "controlID": "C-0260",
            "baseScore": 5.0,
            "category": {
                "name": "Network",
                "id": "Cat-4"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "ensure_network_policy_configured_in_labels",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "ConfigMap"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "networking.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "NetworkPolicy"
                            ]
                        }
                    ],
                    "description": "fails if no networkpolicy configured in workload labels",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\ndeny[msga] {\n\tworkload := input[_]\n\tworkload_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\", \"Pod\", \"CronJob\"}\n\tworkload_kinds[workload.kind]\n\n\tnetworkpolicies := [networkpolicy | networkpolicy = input[_]; networkpolicy.kind == \"NetworkPolicy\"]\n\tnot connected_to_any_network_policy(workload, networkpolicies)\n\t\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"%v: no networkpolicy configured in labels\", [workload.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\":[],\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [workload]\n\t\t}\n\t}\n}\n\n\nconnected_to_any_network_policy(workload, networkpolicies){\n\tconnected_to_network_policy(workload, networkpolicies[_])\n}\n\n# connected_to_network_policy returns true if the workload is connected to the networkpolicy\nconnected_to_network_policy(wl, networkpolicy){\n\tworkload_kinds := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tworkload_kinds[wl.kind]\n\tis_same_namespace(networkpolicy.metadata, wl.metadata)\n\tcount(networkpolicy.spec.podSelector) > 0\n    count({x | networkpolicy.spec.podSelector.matchLabels[x] == wl.spec.template.metadata.labels[x]}) == count(networkpolicy.spec.podSelector.matchLabels)\n}\n\n# connected_to_network_policy returns true if the workload is connected to the networkpolicy\nconnected_to_network_policy(wl, networkpolicy){\n\twl.kind == \"Pod\"\n\tis_same_namespace(networkpolicy.metadata, wl.metadata)\n    count(networkpolicy.spec.podSelector) > 0\n    count({x | networkpolicy.spec.podSelector.matchLabels[x] == wl.metadata.labels[x]}) == count(networkpolicy.spec.podSelector.matchLabels)\n}\n\n# connected_to_network_policy returns true if the workload is connected to the networkpolicy\nconnected_to_network_policy(wl, networkpolicy){\n\twl.kind == \"CronJob\"\n\tis_same_namespace(networkpolicy.metadata, wl.metadata)\n\tcount(networkpolicy.spec.podSelector) > 0\n    count({x | networkpolicy.spec.podSelector.matchLabels[x] == wl.spec.jobTemplate.spec.template.metadata.labels[x]}) == count(networkpolicy.spec.podSelector.matchLabels)\n}\n\n# connected_to_network_policy returns true if the NetworkPolicy has no podSelector.\n# if the NetworkPolicy has no podSelector, it is applied to all workloads in the namespace of the NetworkPolicy\nconnected_to_network_policy(wl, networkpolicy){\n\tis_same_namespace(networkpolicy.metadata, wl.metadata)\n    count(networkpolicy.spec.podSelector) == 0\n}\n\n\nis_same_namespace(metadata1, metadata2) {\n\tmetadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tnot metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata2.namespace\n\tmetadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n\tnot metadata1.namespace\n\tmetadata2.namespace == \"default\"\n}"
                }
            ]
        },
        {
            "name": "ServiceAccount token mounted",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Credential access"
                        ]
                    }
                ]
            },
            "description": "Potential attacker may gain access to a workload and steal its ServiceAccount token. Therefore, it is recommended to disable automatic mapping of the ServiceAccount tokens in ServiceAccount configuration. Enable it only for workloads that need to use them and ensure that this ServiceAccount is not bound to an unnecessary ClusterRoleBinding or RoleBinding.",
            "remediation": "Disable automatic mounting of service account tokens to pods at the workload level, by specifying automountServiceAccountToken: false. Enable it only for workloads that need to use them and ensure that this ServiceAccount doesn't have unnecessary permissions",
            "test": "test if ServiceAccount token is mounted on workload and it has at least one binding.",
            "controlID": "C-0261",
            "baseScore": 7.0,
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "serviceaccount-token-mount",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "ServiceAccount"
                            ]
                        },
                        {
                            "apiGroups": [
                                "rbac.authorization.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "RoleBinding",
                                "ClusterRoleBinding"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "fails if service account and workloads mount service account token by default",
                    "remediation": "Make sure that the automountServiceAccountToken field on the service account spec if set to false",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\ndeny[msga] {\n    wl := input[_]\n    start_of_path := get_beginning_of_path(wl)\n    spec := object.get(wl, start_of_path, [])\n\n    sa := input[_]\n    sa.kind == \"ServiceAccount\"\n    is_same_sa(spec, sa.metadata.name)\n    is_same_namespace(sa.metadata , wl.metadata)\n    has_service_account_binding(sa)\n    result := is_sa_auto_mounted_and_bound(spec, start_of_path, sa)\n\n    failed_path := get_failed_path(result)\n    fixed_path := get_fixed_path(result)\n\n    msga := {\n        \"alertMessage\": sprintf(\"%v: %v in the following namespace: %v mounts service account tokens by default\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n        \"packagename\": \"armo_builtins\",\n        \"alertScore\": 9,\n        \"fixPaths\": fixed_path,\n        \"reviewPaths\": failed_path,\n        \"failedPaths\": failed_path,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n        \"relatedObjects\": [{\n            \"object\": sa\n        }]\n    }\n}\n\n\nget_beginning_of_path(workload) = start_of_path {\n    spec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n    spec_template_spec_patterns[workload.kind]\n    start_of_path := [\"spec\", \"template\", \"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"Pod\"\n    start_of_path := [\"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"CronJob\"\n    start_of_path := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\"]\n}\n\n\n #  -- ----     For workloads     -- ----     \nis_sa_auto_mounted_and_bound(spec, start_of_path, sa) = [failed_path, fix_path]   {\n    # automountServiceAccountToken not in pod spec\n    not spec.automountServiceAccountToken == false\n    not spec.automountServiceAccountToken == true\n\n    not sa.automountServiceAccountToken == false\n\n    fix_path = { \"path\": sprintf(\"%v.automountServiceAccountToken\", [concat(\".\", start_of_path)]), \"value\": \"false\"}\n    failed_path = \"\"\n}\n\nis_sa_auto_mounted_and_bound(spec, start_of_path, sa) =  [failed_path, fix_path]  {\n    # automountServiceAccountToken set to true in pod spec\n    spec.automountServiceAccountToken == true\n\n    failed_path = sprintf(\"%v.automountServiceAccountToken\", [concat(\".\", start_of_path)])\n    fix_path = \"\"\n}\n\nget_failed_path(paths) = [paths[0]] {\n    paths[0] != \"\"\n} else = []\n\n\nget_fixed_path(paths) = [paths[1]] {\n    paths[1] != \"\"\n} else = []\n\n\nis_same_sa(spec, serviceAccountName) {\n    spec.serviceAccountName == serviceAccountName\n}\n\nis_same_sa(spec, serviceAccountName) {\n    not spec.serviceAccountName \n    serviceAccountName == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n    metadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata1.namespace\n    not metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata2.namespace\n    metadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata1.namespace\n    metadata2.namespace == \"default\"\n}\n\n# checks if RoleBinding/ClusterRoleBinding has a bind with the given ServiceAccount\nhas_service_account_binding(service_account) {\n    role_bindings := [role_binding | role_binding = input[_]; endswith(role_binding.kind, \"Binding\")]\n    role_binding := role_bindings[_]\n    role_binding.subjects[_].name == service_account.metadata.name\n    role_binding.subjects[_].namespace == service_account.metadata.namespace\n    role_binding.subjects[_].kind == \"ServiceAccount\"\n}\n\n# checks if RoleBinding/ClusterRoleBinding has a bind with the system:authenticated group\n# which gives access to all authenticated users, including service accounts\nhas_service_account_binding(service_account) {\n    role_bindings := [role_binding | role_binding = input[_]; endswith(role_binding.kind, \"Binding\")]\n    role_binding := role_bindings[_]\n    role_binding.subjects[_].name == \"system:authenticated\"\n}\n\n# checks if RoleBinding/ClusterRoleBinding has a bind with the \"system:serviceaccounts\" group\n# which gives access to all service accounts\nhas_service_account_binding(service_account) {\n    role_bindings := [role_binding | role_binding = input[_]; endswith(role_binding.kind, \"Binding\")]\n    role_binding := role_bindings[_]\n    role_binding.subjects[_].name == \"system:serviceaccounts\"\n}\n",
                    "resourceEnumerator": "package armo_builtins\n\ndeny[msga] {\n    wl := input[_]\n    start_of_path := get_beginning_of_path(wl)\n\n    msga := {\n        \"alertMessage\": sprintf(\"%v: %v in the following namespace: %v mounts service account tokens by default\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n        \"packagename\": \"armo_builtins\",\n        \"alertScore\": 9,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n    }\n}\n\n\nget_beginning_of_path(workload) = start_of_path {\n    spec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n    spec_template_spec_patterns[workload.kind]\n    start_of_path := [\"spec\", \"template\", \"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"Pod\"\n    start_of_path := [\"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"CronJob\"\n    start_of_path := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\"]\n}"
                }
            ]
        },
        {
            "controlID": "C-0262",
            "name": "Anonymous access enabled",
            "description": "Granting permissions to the system:unauthenticated or system:anonymous user is generally not recommended and can introduce security risks. Allowing unauthenticated access to your Kubernetes cluster can lead to unauthorized access, potential data breaches, and abuse of cluster resources.",
            "remediation": "Review and modify your cluster's RBAC configuration to ensure that only authenticated and authorized users have appropriate permissions based on their roles and responsibilities within your system.",
            "test": "Checks if ClusterRoleBinding/RoleBinding resources give permissions to anonymous user. Also checks in the apiserver if the --anonymous-auth flag is set to false",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "baseScore": 7,
            "category": {
                "name": "Control plane",
                "subCategory": {
                    "name": "Supply chain",
                    "id": "Cat-6"
                },
                "id": "Cat-1"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "anonymous-access-enabled",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                "rbac.authorization.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "RoleBinding",
                                "ClusterRoleBinding"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Fails in case anonymous or unauthenticated user has any rbac permissions (is bound by a RoleBinding/ClusterRoleBinding)",
                    "remediation": "Remove any RBAC rules which allow anonymous users to perform actions",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# Fails is rolebinding/clusterrolebinding gives permissions to anonymous user\ndeny[msga] {\n    rolebindings := [rolebinding | rolebinding = input[_]; endswith(rolebinding.kind, \"Binding\")]\n    rolebinding := rolebindings[_]\n    subject := rolebinding.subjects[i]\n    isAnonymous(subject)\n    delete_path := sprintf(\"subjects[%d]\", [i])\n    msga := {\n        \"alertMessage\": sprintf(\"the following RoleBinding: %v gives permissions to anonymous users\", [rolebinding.metadata.name]),\n        \"alertScore\": 9,\n        \"deletePaths\": [delete_path],\n        \"failedPaths\": [delete_path],\n        \"packagename\": \"armo_builtins\",\n        \"alertObject\": {\n            \"k8sApiObjects\": [rolebinding]\n        }\n    }\n}\n\n\nisAnonymous(subject) {\n    subject.name == \"system:anonymous\"\n}\n\nisAnonymous(subject) {\n    subject.name == \"system:unauthenticated\"\n}\n"
                }
            ]
        },
        {
            "name": "PersistentVolume without encyption",
            "attributes": {
                "controlTypeTags": [
                    "security",
                    "compliance"
                ]
            },
            "description": "This control detects PersistentVolumes without encyption",
            "remediation": "Enable encryption on the PersistentVolume using the configuration in StorageClass",
            "test": "Checking all PersistentVolumes via their StorageClass for encryption",
            "controlID": "C-0264",
            "baseScore": 5.0,
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "pv-without-encryption",
                    "attributes": {
                        "useFromKubescapeVersion": "v3.0.3"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "PersistentVolume"
                            ]
                        },
                        {
                            "apiGroups": [
                                "storage.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "StorageClass"
                            ]
                        }
                    ],
                    "description": "PersistentVolume without encryption",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n# Checks if Ingress is connected to a service and a workload to expose something\ndeny[msga] {\n\tpv := input[_]\n\tpv.kind == \"PersistentVolume\"\n\n\t# Find the related storage class\n\tstorageclass := input[_]\n\tstorageclass.kind == \"StorageClass\"\n\tpv.spec.storageClassName == storageclass.metadata.name\n\n\t# Check if storage class is encrypted\n\tnot is_storage_class_encrypted(storageclass)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Volume '%v' has is using a storage class that does not use encryption\", [pv.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": [{\n\t\t\t\"path\": \"spec.storageClassName\",\n\t\t\t\"value\": \"<your encrypted storage class>\"\n        }],\n\t\t\"alertScore\": 7,\n\t\t\"alertObject\": {\"k8sApiObjects\": [pv]}\n\t}\n}\n\n# Storage class is encrypted - AWS\nis_storage_class_encrypted(storageclass) {\n\tstorageclass.parameters.encrypted == \"true\"\n}\n\n# Storage class is encrypted - Azure\nis_storage_class_encrypted(storageclass) {\n\tstorageclass.provisioner\n\tcontains(storageclass.provisioner,\"azure\")\n}\n\n# Storage class is encrypted - GCP\nis_storage_class_encrypted(storageclass) {\n\t# GKE encryption is enabled by default https://cloud.google.com/blog/products/containers-kubernetes/exploring-container-security-use-your-own-keys-to-protect-your-data-on-gke\n\tstorageclass.provisioner\n\tcontains(storageclass.provisioner,\"csi.storage.gke.io\")\n}\n\n"
                }
            ]
        },
        {
            "controlID": "C-0265",
            "name": "Authenticated user has sensitive permissions",
            "description": "Granting permissions to the system:authenticated group is generally not recommended and can introduce security risks. This control ensures that system:authenticated users do not have cluster risking permissions.",
            "remediation": "Review and modify your cluster's RBAC configuration to ensure that system:authenticated will have minimal permissions.",
            "test": "Checks if ClusterRoleBinding/RoleBinding resources give permissions to system:authenticated group.",
            "attributes": {},
            "baseScore": 7,
            "category": {
                "name": "Control plane",
                "subCategory": {
                    "name": "Supply chain",
                    "id": "Cat-6"
                },
                "id": "Cat-1"
            },
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "system-authenticated-allowed-to-take-over-cluster",
                    "attributes": {
                        "resourcesAggregator": "subject-role-rolebinding"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                "rbac.authorization.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "RoleBinding",
                                "ClusterRoleBinding",
                                "Role",
                                "ClusterRole"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "Fails in system:authenticated user has cluster takeover rbac permissions (is bound by a RoleBinding/ClusterRoleBinding)",
                    "remediation": "Remove any RBAC rules which allow system:authenticated users to perform actions",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.in\n\ndeny[msga] {\n    subjectVector := input[_]\n\n\trolebinding := subjectVector.relatedObjects[j]\n\tendswith(rolebinding.kind, \"Binding\")\n\n\n    subject := rolebinding.subjects[k]\n    # Check if the subject is gourp\n    subject.kind == \"Group\"\n    # Check if the subject is system:authenticated\n    subject.name == \"system:authenticated\"\n\n\n    # Find the bound roles\n\trole := subjectVector.relatedObjects[i]\n\tendswith(role.kind, \"Role\")\n\n    # Check if the role and rolebinding bound\n    is_same_role_and_binding(role, rolebinding)\n\n\n    # Check if the role has access to workloads, exec, attach, portforward\n\trule := role.rules[p]\n    rule.resources[l] in [\"*\",\"pods\", \"pods/exec\", \"pods/attach\", \"pods/portforward\",\"deployments\",\"statefulset\",\"daemonset\",\"jobs\",\"cronjobs\",\"nodes\",\"secrets\"]\n\n\tfinalpath := array.concat([\"\"], [\n\t\tsprintf(\"relatedObjects[%d].subjects[%d]\", [j, k]),\n\t\tsprintf(\"relatedObjects[%d].roleRef.name\", [i]),\n\t])\n\n\tmsga := {\n\t\t\"alertMessage\": \"system:authenticated has sensitive roles\",\n\t\t\"alertScore\": 5,\n\t\t\"reviewPaths\": finalpath,\n\t\t\"failedPaths\": finalpath,\n\t\t\"fixPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [],\n            \"externalObjects\" : subjectVector\n\t\t},\n\t}\n}\n\nis_same_role_and_binding(role, rolebinding) {\n    rolebinding.kind == \"RoleBinding\"\n    role.kind == \"Role\"\n    rolebinding.metadata.namespace == role.metadata.namespace\n    rolebinding.roleRef.name == role.metadata.name\n    rolebinding.roleRef.kind == role.kind\n    startswith(role.apiVersion, rolebinding.roleRef.apiGroup)\n}\n\nis_same_role_and_binding(role, rolebinding) {\n    rolebinding.kind == \"ClusterRoleBinding\"\n    role.kind == \"ClusterRole\"\n    rolebinding.roleRef.name == role.metadata.name\n    rolebinding.roleRef.kind == role.kind\n    startswith(role.apiVersion, rolebinding.roleRef.apiGroup)\n}"
                }
            ]
        },
        {
            "name": "Workload with cluster takeover roles",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "external-workload-with-cluster-takeover-roles",
                        "categories": [
                            "Cluster Access"
                        ],
                        "displayRelatedResources": true,
                        "clickableResourceKind": "ServiceAccount"
                    }
                ]
            },
            "description": "Cluster takeover roles include workload creation or update and secret access. They can easily lead to super privileges in the cluster. If an attacker can exploit this workload then the attacker can take over the cluster using the RBAC privileges this workload is assigned to.",
            "remediation": "You should apply least privilege principle. Make sure each service account has only the permissions that are absolutely necessary.",
            "long_description": "In Kubernetes, workloads with overly permissive roles pose a significant security risk. When a workload is granted roles that exceed the necessities of its operation, it creates an attack surface for privilege escalation within the cluster. This is especially critical if the roles include permissions for creating, updating, or accessing sensitive resources or secrets. An attacker exploiting such a workload can leverage these excessive privileges to perform unauthorized actions, potentially leading to a full cluster takeover. Ensuring that each service account associated with a workload is limited to permissions that are strictly necessary for its function is crucial in mitigating the risk of cluster takeovers.",
            "test": "Check if the service account used by a workload has cluster takeover roles.",
            "controlID": "C-0267",
            "baseScore": 6.0,
            "category": {
                "name": "Workload",
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "workload-with-cluster-takeover-roles",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "ServiceAccount"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "rbac.authorization.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "RoleBinding",
                                "ClusterRoleBinding",
                                "Role",
                                "ClusterRole"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.in\n\ndeny[msga] {\n    wl := input[_]\n    start_of_path := get_start_of_path(wl)\n    wl_spec := object.get(wl, start_of_path, [])\n\n    # get service account wl is using\n    sa := input[_]\n    sa.kind == \"ServiceAccount\"\n    is_same_sa(wl_spec, sa.metadata, wl.metadata)\n\n    # check service account token is mounted\n    is_sa_auto_mounted(wl_spec, sa)\n\n    # check if sa has cluster takeover roles\n    role := input[_]\n    role.kind in [\"Role\", \"ClusterRole\"]\n    is_takeover_role(role)\n\n    rolebinding := input[_]\n\trolebinding.kind in [\"RoleBinding\", \"ClusterRoleBinding\"] \n    rolebinding.roleRef.name == role.metadata.name\n    rolebinding.roleRef.kind == role.kind\n    rolebinding.subjects[j].kind == \"ServiceAccount\"\n    rolebinding.subjects[j].name == sa.metadata.name\n    rolebinding.subjects[j].namespace == sa.metadata.namespace\n\n    deletePath := sprintf(\"subjects[%d]\", [j])\n\n    msga := {\n        \"alertMessage\": sprintf(\"%v: %v in the following namespace: %v has cluster takeover roles\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n        \"packagename\": \"armo_builtins\",\n        \"alertScore\": 9,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n        \"relatedObjects\": [{\n            \"object\": sa,\n        },\n        {\n            \"object\": rolebinding,\n            \"deletePaths\": [deletePath],\n        },\n        {\n            \"object\": role,\n        },]\n    }\n}\n\n\nget_start_of_path(workload) = start_of_path {\n    spec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n    spec_template_spec_patterns[workload.kind]\n    start_of_path := [\"spec\", \"template\", \"spec\"]\n}\n\nget_start_of_path(workload) = start_of_path {\n    workload.kind == \"Pod\"\n    start_of_path := [\"spec\"]\n}\n\nget_start_of_path(workload) = start_of_path {\n    workload.kind == \"CronJob\"\n    start_of_path := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\"]\n}\n\n\nis_sa_auto_mounted(wl_spec, sa)    {\n    # automountServiceAccountToken not in pod spec\n    not wl_spec.automountServiceAccountToken == false\n    not wl_spec.automountServiceAccountToken == true\n\n    not sa.automountServiceAccountToken == false\n}\n\nis_sa_auto_mounted(wl_spec, sa)  {\n    # automountServiceAccountToken set to true in pod spec\n    wl_spec.automountServiceAccountToken == true\n}\n\n\nis_same_sa(wl_spec, sa_metadata, wl_metadata) {\n    wl_spec.serviceAccountName == sa_metadata.name\n    is_same_namespace(sa_metadata , wl_metadata)\n}\n\nis_same_sa(wl_spec, sa_metadata, wl_metadata) {\n    not wl_spec.serviceAccountName \n    sa_metadata.name == \"default\"\n    is_same_namespace(sa_metadata , wl_metadata)\n}\n\n# is_same_namespace supports cases where ns is not configured in the metadata\n# for yaml scans\nis_same_namespace(metadata1, metadata2) {\n    metadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata1.namespace\n    not metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata2.namespace\n    metadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata1.namespace\n    metadata2.namespace == \"default\"\n}\n\n\n# look for rule allowing create/update workloads\nis_takeover_role(role){\n    takeover_resources := [\"pods\", \"*\"]\n    takeover_verbs := [\"create\", \"update\", \"patch\", \"*\"]\n    takeover_api_groups := [\"\", \"*\"]\n    \n    takeover_rule := [rule | rule = role.rules[i] ; \n                        rule.resources[a] in takeover_resources ; \n                        rule.verbs[b] in takeover_verbs ; \n                        rule.apiGroups[c] in takeover_api_groups]\n    count(takeover_rule) > 0\n}\n\n# look for rule allowing secret access\nis_takeover_role(role){\n    rule := role.rules[i]\n    takeover_resources := [\"secrets\", \"*\"]\n    takeover_verbs :=  [\"get\", \"list\", \"watch\", \"*\"]\n    takeover_api_groups := [\"\", \"*\"]\n    \n    takeover_rule := [rule | rule = role.rules[i] ; \n                        rule.resources[a] in takeover_resources ; \n                        rule.verbs[b] in takeover_verbs ; \n                        rule.apiGroups[c] in takeover_api_groups]\n    count(takeover_rule) > 0\n}",
                    "resourceEnumerator": "package armo_builtins\n\ndeny[msga] {\n    wl := input[_]\n    start_of_path := get_beginning_of_path(wl)\n\n    msga := {\n        \"alertMessage\": sprintf(\"%v: %v in the following namespace: %v mounts service account tokens by default\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n        \"packagename\": \"armo_builtins\",\n        \"alertScore\": 9,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n    }\n}\n\n\nget_beginning_of_path(workload) = start_of_path {\n    spec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n    spec_template_spec_patterns[workload.kind]\n    start_of_path := [\"spec\", \"template\", \"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"Pod\"\n    start_of_path := [\"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"CronJob\"\n    start_of_path := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\"]\n}"
                }
            ]
        },
        {
            "name": "Ensure CPU limits are set",
            "attributes": {
                "controlTypeTags": [
                    "compliance",
                    "devops",
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "service-destruction",
                        "categories": [
                            "Denial of service"
                        ]
                    }
                ]
            },
            "description": "This control identifies all Pods for which the CPU limits are not set.",
            "remediation": "Set the CPU limits or use exception mechanism to avoid unnecessary notifications.",
            "controlID": "C-0270",
            "baseScore": 8.0,
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Resource management",
                    "id": "Cat-7"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "resources-cpu-limits",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "CPU limits are not set.",
                    "remediation": "Ensure CPU limits are set.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n\n# ==================================== no CPU limits =============================================\n# Fails if pod does not have container with CPU-limits\ndeny[msga] {\n    pod := input[_]\n    pod.kind == \"Pod\"\n    container := pod.spec.containers[i]\n\tnot container.resources.limits.cpu\n\n\tfixPaths := [{\"path\": sprintf(\"spec.containers[%v].resources.limits.cpu\", [format_int(i, 10)]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v does not have CPU-limit or request\", [ container.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [pod]\n\t\t}\n\t}\n}\n\n# Fails if workload does not have container with CPU-limits\ndeny[msga] {\n    wl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n    container := wl.spec.template.spec.containers[i]\n    not container.resources.limits.cpu\n\n\tfixPaths := [{\"path\": sprintf(\"spec.template.spec.containers[%v].resources.limits.cpu\", [format_int(i, 10)]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v in %v: %v   does not have CPU-limit or request\", [ container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n# Fails if cronjob does not have container with CPU-limits\ndeny[msga] {\n  \twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n    not container.resources.limits.cpu\n\n\tfixPaths := [{\"path\": sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].resources.limits.cpu\", [format_int(i, 10)]), \"value\": \"YOUR_VALUE\"}]\n\n    msga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v in %v: %v   does not have CPU-limit or request\", [ container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n\t\t\"fixPaths\": fixPaths,\n\t\t\"alertObject\": {\n\t\t\t\"k8sApiObjects\": [wl]\n\t\t}\n\t}\n}\n\n\n"
                }
            ]
        },
        {
            "name": "Ensure memory limits are set",
            "attributes": {
                "controlTypeTags": [
                    "compliance",
                    "devops",
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "service-destruction",
                        "categories": [
                            "Denial of service"
                        ]
                    }
                ]
            },
            "description": "This control identifies all Pods for which the memory limits are not set.",
            "remediation": "Set the memory limits or use exception mechanism to avoid unnecessary notifications.",
            "controlID": "C-0271",
            "baseScore": 8.0,
            "category": {
                "name": "Workload",
                "subCategory": {
                    "name": "Resource management",
                    "id": "Cat-7"
                },
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "resources-memory-limits",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "memory limits are not set.",
                    "remediation": "Ensure memory limits are set.",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\n#  ================================== no memory limits ==================================\n# Fails if pod does not have container with memory-limits\ndeny[msga] {\n\tpod := input[_]\n\tpod.kind == \"Pod\"\n\tcontainer := pod.spec.containers[i]\n\tnot container.resources.limits.memory\n\tfixPaths := [{\"path\": sprintf(\"spec.containers[%v].resources.limits.memory\", [format_int(i, 10)]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v does not have memory-limit or request\", [container.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"failedPaths\": [],\n\t\t\"alertObject\": {\"k8sApiObjects\": [pod]},\n\t}\n}\n\n# Fails if workload does not have container with memory-limits\ndeny[msga] {\n\twl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Job\"}\n\tspec_template_spec_patterns[wl.kind]\n\tcontainer := wl.spec.template.spec.containers[i]\n\tnot container.resources.limits.memory\n\tfixPaths := [{\"path\": sprintf(\"spec.template.spec.containers[%v].resources.limits.memory\", [format_int(i, 10)]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v in %v: %v   does not have memory-limit or request\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"failedPaths\": [],\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\n# Fails if cronjob does not have container with memory-limits\ndeny[msga] {\n\twl := input[_]\n\twl.kind == \"CronJob\"\n\tcontainer = wl.spec.jobTemplate.spec.template.spec.containers[i]\n\tnot container.resources.limits.memory\n\tfixPaths := [{\"path\": sprintf(\"spec.jobTemplate.spec.template.spec.containers[%v].resources.limits.memory\", [format_int(i, 10)]), \"value\": \"YOUR_VALUE\"}]\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Container: %v in %v: %v   does not have memory-limit or request\", [container.name, wl.kind, wl.metadata.name]),\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": fixPaths,\n\t\t\"failedPaths\": [],\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n"
                }
            ]
        },
        {
            "name": "Workload with administrative roles",
            "attributes": {},
            "description": "This control identifies workloads where the associated service accounts have roles that grant administrative-level access across the cluster. Granting a workload such expansive permissions equates to providing it cluster admin roles. This level of access can pose a significant security risk, as it allows the workload to perform any action on any resource, potentially leading to unauthorized data access or cluster modifications.",
            "remediation": "You should apply least privilege principle. Make sure cluster admin permissions are granted only when it is absolutely necessary. Don't use service accounts with such high permissions for daily operations.",
            "long_description": "In Kubernetes environments, workloads granted administrative-level privileges without restrictions represent a critical security vulnerability. When a service account associated with a workload is configured with permissions to perform any action on any resource, it essentially holds unrestricted access within the cluster, akin to cluster admin privileges. This configuration dramatically increases the risk of security breaches, including data theft, unauthorized modifications, and potentially full cluster takeovers. Such privileges allow attackers to exploit the workload for wide-ranging malicious activities, bypassing the principle of least privilege. Therefore, it's essential to follow the least privilege principle and make sure cluster admin permissions are granted only when it is absolutely necessary.",
            "test": "Check if the service account used by a workload has cluster admin roles, either by being bound to the cluster-admin clusterrole, or by having equivalent high privileges.",
            "controlID": "C-0272",
            "baseScore": 6.0,
            "category": {
                "name": "Workload",
                "id": "Cat-5"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "workload-with-administrative-roles",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "ServiceAccount"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "rbac.authorization.k8s.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "RoleBinding",
                                "ClusterRoleBinding",
                                "Role",
                                "ClusterRole"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.in\n\ndeny[msga] {\n    wl := input[_]\n    start_of_path := get_start_of_path(wl)\n    wl_spec := object.get(wl, start_of_path, [])\n\n    # get service account wl is using\n    sa := input[_]\n    sa.kind == \"ServiceAccount\"\n    is_same_sa(wl_spec, sa.metadata, wl.metadata)\n\n    # check service account token is mounted\n    is_sa_auto_mounted(wl_spec, sa)\n\n    # check if sa has administrative roles\n    role := input[_]\n    role.kind in [\"Role\", \"ClusterRole\"]\n    is_administrative_role(role)\n\n    rolebinding := input[_]\n\trolebinding.kind in [\"RoleBinding\", \"ClusterRoleBinding\"] \n    rolebinding.roleRef.name == role.metadata.name\n    rolebinding.subjects[j].kind == \"ServiceAccount\"\n    rolebinding.subjects[j].name == sa.metadata.name\n    rolebinding.subjects[j].namespace == sa.metadata.namespace\n\n    reviewPath := \"roleRef\"\n    deletePath := sprintf(\"subjects[%d]\", [j])\n\n    msga := {\n        \"alertMessage\": sprintf(\"%v: %v in the following namespace: %v has administrative roles\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n        \"packagename\": \"armo_builtins\",\n        \"alertScore\": 9,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n        \"relatedObjects\": [{\n            \"object\": sa,\n        },\n        {\n            \"object\": rolebinding,\n\t\t    \"reviewPaths\": [reviewPath],\n            \"deletePaths\": [deletePath],\n        },\n        {\n            \"object\": role,\n        },]\n    }\n}\n\n\nget_start_of_path(workload) = start_of_path {\n    spec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n    spec_template_spec_patterns[workload.kind]\n    start_of_path := [\"spec\", \"template\", \"spec\"]\n}\n\nget_start_of_path(workload) = start_of_path {\n    workload.kind == \"Pod\"\n    start_of_path := [\"spec\"]\n}\n\nget_start_of_path(workload) = start_of_path {\n    workload.kind == \"CronJob\"\n    start_of_path := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\"]\n}\n\n\nis_sa_auto_mounted(wl_spec, sa)    {\n    # automountServiceAccountToken not in pod spec\n    not wl_spec.automountServiceAccountToken == false\n    not wl_spec.automountServiceAccountToken == true\n\n    not sa.automountServiceAccountToken == false\n}\n\nis_sa_auto_mounted(wl_spec, sa)  {\n    # automountServiceAccountToken set to true in pod spec\n    wl_spec.automountServiceAccountToken == true\n}\n\n\nis_same_sa(wl_spec, sa_metadata, wl_metadata) {\n    wl_spec.serviceAccountName == sa_metadata.name\n    is_same_namespace(sa_metadata , wl_metadata)\n}\n\nis_same_sa(wl_spec, sa_metadata, wl_metadata) {\n    not wl_spec.serviceAccountName \n    sa_metadata.name == \"default\"\n    is_same_namespace(sa_metadata , wl_metadata)\n}\n\n# is_same_namespace supports cases where ns is not configured in the metadata\n# for yaml scans\nis_same_namespace(metadata1, metadata2) {\n    metadata1.namespace == metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata1.namespace\n    not metadata2.namespace\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata2.namespace\n    metadata1.namespace == \"default\"\n}\n\nis_same_namespace(metadata1, metadata2) {\n    not metadata1.namespace\n    metadata2.namespace == \"default\"\n}\n\n\nis_administrative_role(role){\n    administrative_resources := [\"*\"]\n    administrative_verbs := [\"*\"]\n    administrative_api_groups := [\"\", \"*\"]\n    \n    administrative_rule := [rule | rule = role.rules[i] ; \n                        rule.resources[a] in administrative_resources ; \n                        rule.verbs[b] in administrative_verbs ; \n                        rule.apiGroups[c] in administrative_api_groups]\n    count(administrative_rule) > 0\n}\n",
                    "resourceEnumerator": "package armo_builtins\n\ndeny[msga] {\n    wl := input[_]\n    start_of_path := get_beginning_of_path(wl)\n\n    msga := {\n        \"alertMessage\": sprintf(\"%v: %v in the following namespace: %v mounts service account tokens by default\", [wl.kind, wl.metadata.name, wl.metadata.namespace]),\n        \"packagename\": \"armo_builtins\",\n        \"alertScore\": 9,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n    }\n}\n\n\nget_beginning_of_path(workload) = start_of_path {\n    spec_template_spec_patterns := {\"Deployment\",\"ReplicaSet\",\"DaemonSet\",\"StatefulSet\",\"Job\"}\n    spec_template_spec_patterns[workload.kind]\n    start_of_path := [\"spec\", \"template\", \"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"Pod\"\n    start_of_path := [\"spec\"]\n}\n\nget_beginning_of_path(workload) = start_of_path {\n    workload.kind == \"CronJob\"\n    start_of_path := [\"spec\", \"jobTemplate\", \"spec\", \"template\", \"spec\"]\n}"
                }
            ]
        },
        {
            "name": "Outdated Kubernetes version",
            "attributes": {},
            "description": "Identifies Kubernetes clusters running on outdated versions. Using old versions can expose clusters to known vulnerabilities, compatibility issues, and miss out on improved features and security patches. Keeping Kubernetes up-to-date is crucial for maintaining security and operational efficiency.",
            "remediation": "Regularly update Kubernetes clusters to the latest stable version to mitigate known vulnerabilities and enhance functionality. Plan and execute upgrades considering workload compatibility, testing in a staging environment before applying changes to production. Follow Kubernetes' best practices for version management and upgrades to ensure a smooth transition and minimal downtime.",
            "long_description": "Running an outdated version of Kubernetes poses significant security risks and operational challenges. Older versions may contain unpatched vulnerabilities, leading to potential security breaches and unauthorized access. Additionally, outdated clusters might not support newer, more secure, and efficient features, impacting both performance and security. Regularly updating Kubernetes ensures compliance with the latest security standards and access to enhanced functionalities.",
            "test": "Verifies the current Kubernetes version against the latest stable releases.",
            "controlID": "C-0273",
            "baseScore": 2.0,
            "category": {
                "name": "Control plane",
                "id": "Cat-1"
            },
            "scanningScope": {
                "matches": [
                    "cluster",
                    "file"
                ]
            },
            "rules": [
                {
                    "name": "outdated-k8s-version",
                    "attributes": {},
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Node"
                            ]
                        }
                    ],
                    "ruleDependencies": [],
                    "description": "",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.every\n\ndeny[msga] {\n\tnode := input[_]\n\tnode.kind == \"Node\"\n\tcurrent_version := node.status.nodeInfo.kubeletVersion\n    has_outdated_version(current_version)\n\tpath := \"status.nodeInfo.kubeletVersion\"\n\tmsga := {\n\t\t\t\"alertMessage\": sprintf(\"Your kubelet version: %s, in node: %s is outdated\", [current_version, node.metadata.name]),\n\t\t\t\"reviewPaths\": [path],\n\t\t\t\"alertObject\": {\"k8SApiObjects\": [node]},\n\t}\n}\n\n\nhas_outdated_version(version)  {\n\t# the `supported_k8s_versions` is validated in the validations script against \"https://api.github.com/repos/kubernetes/kubernetes/releases\"\n    supported_k8s_versions := [\"v1.31\", \"v1.30\", \"v1.29\"] \n\tevery v in supported_k8s_versions{\n\t\tnot startswith(version, v)\n\t}\n}\n"
                }
            ]
        },
        {
            "name": "Exposure to internet via Gateway API",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-external-track",
                        "categories": [
                            "Initial Access"
                        ]
                    },
                    {
                        "attackTrack": "service-destruction",
                        "categories": [
                            "Initial Access"
                        ]
                    },
                    {
                        "attackTrack": "external-workload-with-cluster-takeover-roles",
                        "categories": [
                            "Initial Access"
                        ]
                    },
                    {
                        "attackTrack": "workload-unauthenticated-service",
                        "categories": [
                            "Initial Access"
                        ]
                    }
                ]
            },
            "description": "This control detect workloads that are exposed on Internet through a Gateway API (HTTPRoute,TCPRoute, UDPRoute) or Istio Gateway. It fails in case it find workloads connected with these resources.",
            "remediation": "The user can evaluate its exposed resources and apply relevant changes wherever needed.",
            "test": "Checks if workloads are exposed through the use of Gateway API (HTTPRoute,TCPRoute, UDPRoute) or Istio Gateway.",
            "controlID": "C-0266",
            "baseScore": 7.0,
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "exposure-to-internet-via-gateway-api",
                    "attributes": {
                        "useFromKubescapeVersion": "v3.0.9"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "Service"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "gateway.networking.k8s.io"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "HTTPRoute",
                                "TCPRoute",
                                "UDPRoute"
                            ]
                        }
                    ],
                    "description": "fails if the running workload is bound to a Service that is exposed to the Internet through a Gateway.",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\nimport future.keywords.in\n\n\ndeny[msga] {\n    httproute := input[_]\n    httproute.kind in [\"HTTPRoute\", \"TCPRoute\", \"UDPRoute\"]\n\n    svc := input[_]\n    svc.kind == \"Service\"\n\n    # Make sure that they belong to the same namespace\n    svc.metadata.namespace == httproute.metadata.namespace\n\n    # avoid duplicate alerts\n    # if service is already exposed through NodePort or LoadBalancer workload will fail on that\n    not is_exposed_service(svc)\n\n    wl := input[_]\n    wl.metadata.namespace == svc.metadata.namespace\n    spec_template_spec_patterns := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Pod\", \"Job\", \"CronJob\"}\n    spec_template_spec_patterns[wl.kind]\n    wl_connected_to_service(wl, svc)\n\n    result := svc_connected_to_httproute(svc, httproute)\n\n    msga := {\n        \"alertMessage\": sprintf(\"workload '%v' is exposed through httproute '%v'\", [wl.metadata.name, httproute.metadata.name]),\n        \"packagename\": \"armo_builtins\",\n        \"failedPaths\": [],\n        \"fixPaths\": [],\n        \"alertScore\": 7,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n        \"relatedObjects\": [\n\t\t{\n\t            \"object\": httproute,\n\t\t    \"reviewPaths\": result,\n\t            \"failedPaths\": result,\n\t        },\n\t\t{\n\t            \"object\": svc,\n\t\t}\n        ]\n    }\n}\n\n# ====================================================================================\n\nis_exposed_service(svc) {\n    svc.spec.type == \"NodePort\"\n}\n\nis_exposed_service(svc) {\n    svc.spec.type == \"LoadBalancer\"\n}\n\nwl_connected_to_service(wl, svc) {\n    count({x | svc.spec.selector[x] == wl.metadata.labels[x]}) == count(svc.spec.selector)\n}\n\nwl_connected_to_service(wl, svc) {\n    wl.spec.selector.matchLabels == svc.spec.selector\n}\n\nwl_connected_to_service(wl, svc) {\n    count({x | svc.spec.selector[x] == wl.spec.template.metadata.labels[x]}) == count(svc.spec.selector)\n}\n\nsvc_connected_to_httproute(svc, httproute) = result {\n    rule := httproute.spec.rules[i]\n    ref := rule.backendRefs[j]\n    ref.kind == \"Service\"\n    svc.metadata.name == ref.name\n    result := [sprintf(\"spec.rules[%d].backendRefs[%d].name\", [i,j])]\n}\n\n"
                },
                {
                    "name": "exposure-to-internet-via-istio-ingress",
                    "attributes": {
                        "useFromKubescapeVersion": "v3.0.9"
                    },
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "Service"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "networking.istio.io"
                            ],
                            "apiVersions": [
                                "v1",
                                "v1beta1"
                            ],
                            "resources": [
                                "VirtualService",
                                "Gateways"
                            ]
                        }
                    ],
                    "description": "fails if the running workload is bound to a Service that is exposed to the Internet through Istio Gateway.",
                    "remediation": "",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\nimport future.keywords.in\n\n\ndeny[msga] {\n    virtualservice := input[_]\n    virtualservice.kind == \"VirtualService\"\n\n    # Get the namescape of the VirtualService\n    vs_ns := get_namespace(virtualservice)\n    # Looping over the gateways of the VirtualService\n    vs_gw_name := virtualservice.spec.gateways[_]\n    # Get the namespace of the Gateway\n    vs_gw = get_vs_gw_ns(vs_ns, vs_gw_name)\n\n    # Check if the VirtualService is connected to a Gateway\n    gateway := input[_]\n    gateway.kind == \"Gateway\"\n    gateway.metadata.name == vs_gw.name\n    get_namespace(gateway) == vs_gw.namespace\n\n    # print(\"Found the gateway that the virtualservice is connected to\", gateway)\n\n    # Either the gateway is exposed via LoadBalancer/n service OR has \"public\" suffix\n    is_gateway_public(gateway, input)\n\n    # print(\"Gateway is public\", gateway)\n\n    # Check if the VirtualService is connected to an workload\n    # First, find the service that the VirtualService is connected to\n    connected_service := input[_]\n    connected_service.kind == \"Service\"\n    fqsn := get_fqsn(get_namespace(virtualservice), virtualservice.spec.http[i].route[j].destination.host)\n    target_ns := split(fqsn,\".\")[1]\n    target_name := split(fqsn,\".\")[0]\n    # Check if the service is in the same namespace as the VirtualService\n    get_namespace(connected_service) == target_ns\n    # Check if the service is the target of the VirtualService\n    connected_service.metadata.name == target_name\n\n    # print(\"Found the service that the virtualservice is connected to\", connected_service)\n\n    # Check if the service is connected to a workload\n    wl := input[_]\n    is_same_namespace(connected_service, wl)\n    spec_template_spec_patterns := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Pod\", \"Job\", \"CronJob\"}\n    spec_template_spec_patterns[wl.kind]\n    wl_connected_to_service(wl, connected_service)\n\n    # print(\"Found the workload that the service is connected to\", wl)\n\n    failedPaths := [sprintf(\"spec.http[%d].routes[%d].destination.host\", [i,j])]\n\n    # print(\"Found the failed paths\", failedPaths)\n\n    msga := {\n        \"alertMessage\": sprintf(\"workload '%v' is exposed through virtualservice '%v'\", [wl.metadata.name, virtualservice.metadata.name]),\n        \"packagename\": \"armo_builtins\",\n        \"failedPaths\": [],\n        \"fixPaths\": [],\n        \"alertScore\": 7,\n        \"alertObject\": {\n            \"k8sApiObjects\": [wl]\n        },\n        \"relatedObjects\": [\n\t\t    {\n\t            \"object\": virtualservice,\n\t            \"reviewPaths\": failedPaths,\n\t            \"failedPaths\": failedPaths,\n\t        },\n            {\n                    \"object\": connected_service,\n            }\n        ]\n    }\n}\n\n# ====================================================================================\n\nis_gateway_public(gateway, inputs) {\n    endswith(gateway.metadata.name, \"public\")\n}\n\nis_gateway_public(gateway, inputs) {\n    inputs[_].kind == \"Service\"\n    inputs[_].metadata.namespace == \"istio-system\"\n    gateway.spec.selector[_] == inputs[_].metadata.labels[_]\n    is_exposed_service(inputs[_])\n}\n\n\nget_namespace(obj) = namespace {\n    obj.metadata\n    obj.metadata.namespace\n    namespace := obj.metadata.namespace\n}\n\nget_namespace(obj) = namespace {\n    not obj.metadata.namespace\n    namespace := \"default\"\n}\n\nget_vs_gw_ns(vs_ns, vs_gw_name) = {\"name\": name, \"namespace\": ns} {\n    # Check if there is a / in the gateway name\n    count(split(vs_gw_name, \"/\")) == 2\n    ns := split(vs_gw_name, \"/\")[0]\n    name := split(vs_gw_name, \"/\")[1]\n}\n\nget_vs_gw_ns(vs_ns, vs_gw_name) = {\"name\": name, \"namespace\": ns} {\n    # Check if there is no / in the gateway name\n    count(split(vs_gw_name, \"/\")) == 1\n    ns := vs_ns\n    name := vs_gw_name\n}\n\nis_same_namespace(obj1, obj2) {\n    obj1.metadata.namespace == obj2.metadata.namespace\n}\n\nis_same_namespace(obj1, obj2) {\n    not obj1.metadata.namespace\n    obj2.metadata.namespace == \"default\"\n}\n\nis_same_namespace(obj1, obj2) {\n    not obj2.metadata.namespace\n    obj1.metadata.namespace == \"default\"\n}\n\nis_same_namespace(obj1, obj2) {\n    not obj1.metadata.namespace\n    not obj2.metadata.namespace\n}\n\nis_exposed_service(svc) {\n    svc.spec.type == \"NodePort\"\n}\n\nis_exposed_service(svc) {\n    svc.spec.type == \"LoadBalancer\"\n}\n\nwl_connected_to_service(wl, svc) {\n    count({x | svc.spec.selector[x] == wl.metadata.labels[x]}) == count(svc.spec.selector)\n}\n\nwl_connected_to_service(wl, svc) {\n    wl.spec.selector.matchLabels == svc.spec.selector\n}\n\nwl_connected_to_service(wl, svc) {\n    count({x | svc.spec.selector[x] == wl.spec.template.metadata.labels[x]}) == count(svc.spec.selector)\n}\n\nsvc_connected_to_virtualservice(svc, virtualservice) = result {\n    host := virtualservice.spec.http[i].route[j].destination.host\n    svc.metadata.name == host\n    result := [sprintf(\"spec.http[%d].routes[%d].destination.host\", [i,j])]\n}\n\nget_fqsn(ns, dest_host) = fqsn {\n    # verify that this name is without the namespace\n    count(split(\".\", dest_host)) == 1\n    fqsn := sprintf(\"%v.%v.svc.cluster.local\", [dest_host, ns])\n}\n\nget_fqsn(ns, dest_host) = fqsn {\n    count(split(\".\", dest_host)) == 2\n    fqsn := sprintf(\"%v.svc.cluster.local\", [dest_host])\n}\n\nget_fqsn(ns, dest_host) = fqsn {\n    count(split(\".\", dest_host)) == 4\n    fqsn := dest_host\n}\n\n\n"
                }
            ]
        },
        {
            "name": "Verify Authenticated Service",
            "controlID": "C-0274",
            "description": "Verifies if the service is authenticated",
            "long_description": "Verifies that in order to access the service, the user must be authenticated.",
            "remediation": "Configure the service to require authentication.",
            "manual_test": "",
            "attributes": {
                "controlTypeTags": [
                    "security"
                ],
                "attackTracks": [
                    {
                        "attackTrack": "workload-unauthenticated-service",
                        "categories": [
                            "Execution"
                        ]
                    }
                ]
            },
            "baseScore": 7,
            "impact_statement": "",
            "default_value": "",
            "category": {
                "name": "Network",
                "id": "Cat-4"
            },
            "scanningScope": {
                "matches": [
                    "cluster"
                ]
            },
            "rules": [
                {
                    "name": "unauthenticated-service",
                    "ruleLanguage": "Rego",
                    "match": [
                        {
                            "apiGroups": [
                                ""
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Pod",
                                "Service"
                            ]
                        },
                        {
                            "apiGroups": [
                                "apps"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "Deployment",
                                "ReplicaSet",
                                "DaemonSet",
                                "StatefulSet"
                            ]
                        },
                        {
                            "apiGroups": [
                                "batch"
                            ],
                            "apiVersions": [
                                "*"
                            ],
                            "resources": [
                                "Job",
                                "CronJob"
                            ]
                        },
                        {
                            "apiGroups": [
                                "kubescape.io"
                            ],
                            "apiVersions": [
                                "v1"
                            ],
                            "resources": [
                                "servicesscanresults"
                            ]
                        }
                    ],
                    "dynamicMatch": [],
                    "ruleDependencies": [],
                    "description": "Verifies that the service is authenticated",
                    "remediation": "Add authentication to the service",
                    "ruleQuery": "armo_builtins",
                    "rule": "package armo_builtins\n\nimport future.keywords.contains\nimport future.keywords.if\n\ndeny contains msga if {\n\tservice := input[_]\n\tservice.kind == \"Service\"\n\n\twl := input[_]\n\tspec_template_spec_patterns := {\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Pod\", \"Job\", \"CronJob\"}\n\tspec_template_spec_patterns[wl.kind]\n\twl_connected_to_service(wl, service)\n\n\tservice_scan_result := input[_]\n\tservice_scan_result.kind == \"ServiceScanResult\"\n\tservice_name := service.metadata.name\n\thas_unauthenticated_service(service_name, service.metadata.namespace, service_scan_result)\n\n\tmsga := {\n\t\t\"alertMessage\": sprintf(\"Unauthenticated service %v exposes %v\", [service_name, wl.metadata.name]),\n\t\t\"alertScore\": 7,\n\t\t\"fixPaths\": [],\n\t\t\"reviewPaths\": [],\n\t\t\"failedPaths\": [],\n\t\t\"packagename\": \"armo_builtins\",\n\t\t\"alertObject\": {\"k8sApiObjects\": [wl]},\n\t}\n}\n\nhas_unauthenticated_service(service_name, namespace, service_scan_result) if {\n\tservice_scan_result.metadata.name == service_name\n\tservice_scan_result.metadata.namespace == namespace\n\tservice_scan_result.spec.ports[_].authenticated == false\n}\n\nwl_connected_to_service(wl, svc) if {\n\tcount({x | svc.spec.selector[x] == wl.metadata.labels[x]}) == count(svc.spec.selector)\n}\n\nwl_connected_to_service(wl, svc) if {\n\twl.spec.selector.matchLabels == svc.spec.selector\n}\n"
                }
            ]
        }
    ],
    "ControlsIDs": [
        "C-0005",
        "C-0012",
        "C-0013",
        "C-0016",
        "C-0017",
        "C-0034",
        "C-0035",
        "C-0038",
        "C-0041",
        "C-0044",
        "C-0045",
        "C-0046",
        "C-0048",
        "C-0057",
        "C-0066",
        "C-0069",
        "C-0070",
        "C-0074",
        "C-0211",
        "C-0255",
        "C-0256",
        "C-0257",
        "C-0258",
        "C-0259",
        "C-0260",
        "C-0261",
        "C-0262",
        "C-0264",
        "C-0265",
        "C-0267",
        "C-0270",
        "C-0271",
        "C-0272",
        "C-0273",
        "C-0266",
        "C-0274"
    ]
}